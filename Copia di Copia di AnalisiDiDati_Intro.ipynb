{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "wuIvpvlE6edI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Caricamento libri\n"
   ],
   "metadata": {
    "id": "cWjP7AbSguRx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx8AKCuZZWn9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757069503698,
     "user_tz": -120,
     "elapsed": 1040,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "14757afc-af8e-4a5d-d512-627bdd072a83"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ijdKI05sOxog",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1d735a69-c569-4e9c-8aba-f90200823854",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757071495228,
     "user_tz": -120,
     "elapsed": 198,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "primo autore: 5 libri, 2,274,448 caratteri\n",
      "secondo autore: 7 libri, 3,173,605 caratteri\n",
      "terzo autore: 8 libri, 2,208,855 caratteri\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def estrai_contenuto_gutenberg(testo):\n",
    "    pattern_start = r'\\*\\*\\* START OF (?:THE |THIS )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*'\n",
    "    pattern_end = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK '\n",
    "\n",
    "    match_start = re.search(pattern_start, testo, re.IGNORECASE | re.DOTALL)\n",
    "    match_end = re.search(pattern_end, testo, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if match_start and match_end:\n",
    "        return testo[match_start.end():match_end.start()].strip()\n",
    "    elif match_start:\n",
    "        return testo[match_start.end():].strip()\n",
    "    elif match_end:\n",
    "        return testo[:match_end.start()].strip()\n",
    "    else:\n",
    "        return testo\n",
    "\n",
    "def carica_tutti_i_libri(path_input, cartella_output):\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    # Crea cartella output se non esistente\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "\n",
    "    libri_raggruppati = {autore: [] for autore in autori}\n",
    "\n",
    "    for autore in autori:\n",
    "        cartella = f\"{path_input}{autore}\"\n",
    "\n",
    "        try:\n",
    "            file_txt = [f for f in os.listdir(cartella)\n",
    "                       if f.endswith('.txt')]\n",
    "\n",
    "            if not file_txt:\n",
    "                continue\n",
    "\n",
    "            file_txt.sort()\n",
    "\n",
    "            for nome_file in file_txt:\n",
    "                percorso_completo = os.path.join(cartella, nome_file)\n",
    "\n",
    "                try:\n",
    "                    with open(percorso_completo, \"r\", encoding=\"utf-8\") as f:\n",
    "                        testo_completo = f.read()\n",
    "\n",
    "                    contenuto_libro = estrai_contenuto_gutenberg(testo_completo)\n",
    "\n",
    "                    if len(contenuto_libro) > 100:\n",
    "                        libri_raggruppati[autore].append(contenuto_libro)\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        except (FileNotFoundError, Exception):\n",
    "            continue\n",
    "\n",
    "    # Salva file combinati\n",
    "    for autore, libri_autore in libri_raggruppati.items():\n",
    "        if libri_autore:\n",
    "            nome_file = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "            percorso_file = os.path.join(cartella_output, nome_file)\n",
    "\n",
    "            with open(percorso_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(libri_autore))\n",
    "\n",
    "    return libri_raggruppati\n",
    "\n",
    "# Esecuzione\n",
    "path_trainings = \"/content/drive/MyDrive/libri_training/\"\n",
    "cartella_output = \"/content/drive/MyDrive/libri_puliti_training\"\n",
    "\n",
    "libri_raggruppati = carica_tutti_i_libri(path_trainings, cartella_output)\n",
    "\n",
    "# Riepilogo essenziale\n",
    "for autore, libri in libri_raggruppati.items():\n",
    "    if libri:\n",
    "        caratteri_totali = sum(len(libro) for libro in libri)\n",
    "        print(f\"{autore}: {len(libri)} libri, {caratteri_totali:,} caratteri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funzione di estrazione paragrafi\n"
   ],
   "metadata": {
    "id": "vkpC7s62gsPU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Preparazione dati: funzioni varie"
   ],
   "metadata": {
    "id": "0YwuLqQt1dBA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def crea_corpus_word_embedding(\n",
    "    cartella_libri_puliti_training,\n",
    "    cartella_libri_puliti_test_val,\n",
    "    cartella_output_finale,\n",
    "    min_words_embedding=50,\n",
    "    max_words_embedding=100,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea corpus unificato per word embedding FastText\n",
    "    Include tutti i paragrafi di tutti gli autori (pi√π permissivo sui limiti)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CREAZIONE CORPUS per WORD EMBEDDING\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Parametri: {min_words_embedding}-{max_words_embedding} parole per paragrafo\")\n",
    "\n",
    "    # Cartella per i file di embedding\n",
    "    cartella_embedding = os.path.join(cartella_output_finale, \"word_embedding_corpus\")\n",
    "    os.makedirs(cartella_embedding, exist_ok=True)\n",
    "\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    # Lista per raccogliere tutti i paragrafi\n",
    "    tutti_paragrafi = []\n",
    "    paragrafi_per_autore = {}\n",
    "\n",
    "    # Processa entrambe le cartelle (training + test/val)\n",
    "    cartelle_source = [\n",
    "        (\"training\", cartella_libri_puliti_training),\n",
    "        (\"test_val\", cartella_libri_puliti_test_val)\n",
    "    ]\n",
    "\n",
    "    for tipo_set, cartella_source in cartelle_source:\n",
    "        print(f\"\\n  Processando set {tipo_set}:\")\n",
    "\n",
    "        for autore in autori:\n",
    "            nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "            file_source = os.path.join(cartella_source, nome_file_autore)\n",
    "\n",
    "            if os.path.exists(file_source):\n",
    "                # Estrai paragrafi con limiti pi√π permissivi\n",
    "                paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                    file_source,\n",
    "                    min_words=min_words_embedding,\n",
    "                    max_words=max_words_embedding,\n",
    "                    debug=debug\n",
    "                )\n",
    "\n",
    "                print(f\"    {autore} ({tipo_set}): {len(paragrafi_autore)} paragrafi\")\n",
    "\n",
    "                # Aggiungi alla collezione totale\n",
    "                tutti_paragrafi.extend(paragrafi_autore)\n",
    "\n",
    "                # Tieni traccia per autore\n",
    "                if autore not in paragrafi_per_autore:\n",
    "                    paragrafi_per_autore[autore] = []\n",
    "                paragrafi_per_autore[autore].extend(paragrafi_autore)\n",
    "            else:\n",
    "                print(f\"    {autore} ({tipo_set}): File non trovato\")\n",
    "\n",
    "    # Salva corpus unificato per FastText\n",
    "    print(f\"\\nSalvataggio corpus per word embedding:\")\n",
    "\n",
    "    # 1. Corpus completo (tutti gli autori insieme)\n",
    "    file_corpus_completo = os.path.join(cartella_embedding, \"corpus_completo.txt\")\n",
    "    with open(file_corpus_completo, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paragrafo in tutti_paragrafi:\n",
    "            # Ogni paragrafo su una riga (standard per FastText)\n",
    "            f.write(paragrafo.replace('\\n', ' ') + '\\n')\n",
    "\n",
    "    print(f\"  ‚úì Corpus completo: {len(tutti_paragrafi)} paragrafi ‚Üí corpus_completo.txt\")\n",
    "\n",
    "    # 2. Corpus separati per autore (opzionale, utile per analisi)\n",
    "    for autore, paragrafi in paragrafi_per_autore.items():\n",
    "        nome_file_autore_corpus = f\"corpus_{autore.replace(' ', '_')}.txt\"\n",
    "        file_corpus_autore = os.path.join(cartella_embedding, nome_file_autore_corpus)\n",
    "\n",
    "        with open(file_corpus_autore, \"w\", encoding=\"utf-8\") as f:\n",
    "            for paragrafo in paragrafi:\n",
    "                f.write(paragrafo.replace('\\n', ' ') + '\\n')\n",
    "\n",
    "        print(f\"  ‚úì {autore}: {len(paragrafi)} paragrafi ‚Üí {nome_file_autore_corpus}\")\n",
    "\n",
    "    # 3. Salva statistiche\n",
    "    tutte_le_parole = []\n",
    "    for paragrafo in tutti_paragrafi:\n",
    "        parole = re.findall(r'\\b\\w+\\b', paragrafo.lower())\n",
    "        tutte_le_parole.extend(parole)\n",
    "\n",
    "    vocabulary_unico = set(tutte_le_parole)\n",
    "\n",
    "    file_stats = os.path.join(cartella_embedding, \"corpus_statistics.txt\")\n",
    "    with open(file_stats, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"STATISTICHE CORPUS WORD EMBEDDING\\n\")\n",
    "        f.write(f\"=====================================\\n\\n\")\n",
    "        f.write(f\"Parametri estrazione: {min_words_embedding}-{max_words_embedding} parole/paragrafo\\n\\n\")\n",
    "        f.write(f\"TOTALI:\\n\")\n",
    "        f.write(f\"  - Paragrafi: {len(tutti_paragrafi):,}\\n\")\n",
    "        f.write(f\"  - Parole totali: {len(tutte_le_parole):,}\\n\")\n",
    "        f.write(f\"  - Vocabulary unico: {len(vocabulary_unico):,}\\n\\n\")\n",
    "        f.write(f\"PER AUTORE:\\n\")\n",
    "        for autore, paragrafi in paragrafi_per_autore.items():\n",
    "            f.write(f\"  - {autore}: {len(paragrafi):,} paragrafi\\n\")\n",
    "\n",
    "    print(f\"\\nStatistiche vocabulary:\")\n",
    "    print(f\"  - Paragrafi totali: {len(tutti_paragrafi):,}\")\n",
    "    print(f\"  - Parole totali: {len(tutte_le_parole):,}\")\n",
    "    print(f\"  - Vocabulary unico: {len(vocabulary_unico):,}\")\n",
    "\n",
    "    return cartella_embedding\n",
    "\n",
    "def conta_parole_correttamente(testo):\n",
    "    \"\"\"Conta le parole in modo accurato usando regex\"\"\"\n",
    "    if not testo or not testo.strip():\n",
    "        return 0\n",
    "\n",
    "    # Rimuovi spazi extra e normalizza\n",
    "    testo_pulito = re.sub(r'\\s+', ' ', testo.strip())\n",
    "\n",
    "    # Usa regex per trovare sequenze di caratteri alfanumerici\n",
    "    parole = re.findall(r'\\b\\w+\\b', testo_pulito)\n",
    "\n",
    "    return len(parole)\n",
    "\n",
    "def pulisci_paragrafo(paragrafo):\n",
    "    \"\"\"Pulisce un paragrafo mantenendo la leggibilit√†\"\"\"\n",
    "    if not paragrafo:\n",
    "        return \"\"\n",
    "\n",
    "    # Rimuovi caratteri di controllo\n",
    "    paragrafo = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', paragrafo)\n",
    "\n",
    "    # Normalizza spazi multipli\n",
    "    paragrafo = re.sub(r'\\s+', ' ', paragrafo)\n",
    "\n",
    "    # Rimuovi spazi all'inizio e fine\n",
    "    paragrafo = paragrafo.strip()\n",
    "\n",
    "    # Rimuovi linee che sono solo punteggiatura o numeri\n",
    "    if re.match(r'^[^\\w]*$', paragrafo) or re.match(r'^\\d+\\.?\\s*$', paragrafo):\n",
    "        return \"\"\n",
    "\n",
    "    return paragrafo\n",
    "\n",
    "def estrai_paragrafi_da_file_corretto(file_path, min_words=50, max_words=100, debug=False):\n",
    "    \"\"\"\n",
    "    Estrae paragrafi validi da un singolo file con conteggio parole accurato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            testo = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore leggendo {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if debug:\n",
    "        print(f\"File: {os.path.basename(file_path)}\")\n",
    "        print(f\"Dimensione originale: {len(testo)} caratteri\")\n",
    "\n",
    "    # Gestisci diversi tipi di separatori di paragrafo\n",
    "    # Prima normalizza i line break\n",
    "    testo = testo.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Identifica paragrafi (linee vuote multiple = separatore paragrafo)\n",
    "    paragrafi_raw = re.split(r'\\n\\s*\\n', testo)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Paragrafi raw trovati: {len(paragrafi_raw)}\")\n",
    "\n",
    "    paragrafi_validi = []\n",
    "    stats = {'troppo_corti': 0, 'troppo_lunghi': 0, 'validi': 0, 'vuoti': 0}\n",
    "\n",
    "    for i, paragrafo_raw in enumerate(paragrafi_raw):\n",
    "        # Pulisci il paragrafo\n",
    "        paragrafo = pulisci_paragrafo(paragrafo_raw)\n",
    "\n",
    "        if not paragrafo:\n",
    "            stats['vuoti'] += 1\n",
    "            continue\n",
    "\n",
    "        # Conta parole accuratamente\n",
    "        num_parole = conta_parole_correttamente(paragrafo)\n",
    "\n",
    "        if debug and i < 5:  # Mostra primi 5 per debug\n",
    "            print(f\"Paragrafo {i+1}: {num_parole} parole - '{paragrafo[:100]}...'\")\n",
    "\n",
    "        # Filtra per lunghezza\n",
    "        if num_parole < min_words:\n",
    "            stats['troppo_corti'] += 1\n",
    "        elif num_parole > max_words:\n",
    "            stats['troppo_lunghi'] += 1\n",
    "        else:\n",
    "            paragrafi_validi.append(paragrafo)\n",
    "            stats['validi'] += 1\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Statistiche estrazione:\")\n",
    "        print(f\"  - Paragrafi validi: {stats['validi']}\")\n",
    "        print(f\"  - Troppo corti (<{min_words}): {stats['troppo_corti']}\")\n",
    "        print(f\"  - Troppo lunghi (>{max_words}): {stats['troppo_lunghi']}\")\n",
    "        print(f\"  - Vuoti/invalidi: {stats['vuoti']}\")\n",
    "\n",
    "    return paragrafi_validi\n",
    "\n",
    "def verifica_paragrafi_estratti(paragrafi, min_words=50, max_words=100):\n",
    "    \"\"\"Verifica che i paragrafi estratti rispettino i limiti\"\"\"\n",
    "    print(f\"\\nVerifica {len(paragrafi)} paragrafi:\")\n",
    "\n",
    "    problemi = []\n",
    "    for i, paragrafo in enumerate(paragrafi):\n",
    "        num_parole = conta_parole_correttamente(paragrafo)\n",
    "\n",
    "        if num_parole < min_words:\n",
    "            problemi.append(f\"Paragrafo {i+1}: {num_parole} parole (< {min_words})\")\n",
    "        elif num_parole > max_words:\n",
    "            problemi.append(f\"Paragrafo {i+1}: {num_parole} parole (> {max_words})\")\n",
    "\n",
    "    if problemi:\n",
    "        print(f\"ATTENZIONE: {len(problemi)} paragrafi fuori range:\")\n",
    "        for problema in problemi[:10]:  # Mostra max 10\n",
    "            print(f\"  - {problema}\")\n",
    "        if len(problemi) > 10:\n",
    "            print(f\"  ... e altri {len(problemi) - 10}\")\n",
    "    else:\n",
    "        print(\"Tutti i paragrafi rispettano i limiti di parole\")\n",
    "\n",
    "    # Statistiche\n",
    "    parole_per_paragrafo = [conta_parole_correttamente(p) for p in paragrafi]\n",
    "    if parole_per_paragrafo:\n",
    "        print(f\"Parole per paragrafo - Min: {min(parole_per_paragrafo)}, \"\n",
    "              f\"Max: {max(parole_per_paragrafo)}, \"\n",
    "              f\"Media: {sum(parole_per_paragrafo)/len(parole_per_paragrafo):.1f}\")\n",
    "\n",
    "def crea_struttura_dataset_finale_corretta(\n",
    "    cartella_libri_puliti_training,\n",
    "    cartella_libri_puliti_test_val,\n",
    "    cartella_output_finale,\n",
    "    min_words=50,\n",
    "    max_words=100,\n",
    "    test_size=0.5,\n",
    "    debug=False,\n",
    "    verifica_output=True,\n",
    "    crea_embedding_corpus=False  # <-- AGGIUNGI QUESTO PARAMETRO\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Versione corretta della creazione dataset con conteggio parole accurato\n",
    "    \"\"\"\n",
    "\n",
    "    # Pulisci cartella output\n",
    "    if os.path.exists(cartella_output_finale):\n",
    "        print(f\"Pulizia cartella output: {cartella_output_finale}\")\n",
    "        import shutil\n",
    "        shutil.rmtree(cartella_output_finale)\n",
    "\n",
    "   # AGGIUNGI QUESTO BLOCCO all'inizio della funzione (dopo la pulizia cartella)\n",
    "    if crea_embedding_corpus:\n",
    "        crea_corpus_word_embedding(\n",
    "            cartella_libri_puliti_training,\n",
    "            cartella_libri_puliti_test_val,\n",
    "            cartella_output_finale,\n",
    "            debug=debug\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CREAZIONE DATASET per CLASSIFICATION\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    # Crea struttura\n",
    "    sets = [\"training_set\", \"test_set\", \"eval_set\"]\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    for set_name in sets:\n",
    "        for autore in autori:\n",
    "            cartella_set_autore = os.path.join(cartella_output_finale, set_name, autore)\n",
    "            os.makedirs(cartella_set_autore, exist_ok=True)\n",
    "\n",
    "    print(f\"Struttura cartelle creata in: {cartella_output_finale}\")\n",
    "\n",
    "    # TRAINING SET\n",
    "    print(f\"\\nProcessing TRAINING SET (parole: {min_words}-{max_words}):\")\n",
    "    training_stats = {}\n",
    "\n",
    "    for autore in autori:\n",
    "        nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "        file_source = os.path.join(cartella_libri_puliti_training, nome_file_autore)\n",
    "\n",
    "        print(f\"\\n  {autore}:\")\n",
    "\n",
    "        if os.path.exists(file_source):\n",
    "            # Estrai paragrafi con nuovo metodo\n",
    "            paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                file_source,\n",
    "                min_words=min_words,\n",
    "                max_words=max_words,\n",
    "                debug=debug\n",
    "            )\n",
    "\n",
    "            if paragrafi_autore:\n",
    "                # Verifica se richiesto\n",
    "                if verifica_output:\n",
    "                    verifica_paragrafi_estratti(paragrafi_autore, min_words, max_words)\n",
    "\n",
    "                # Salva paragrafi\n",
    "                cartella_training_autore = os.path.join(cartella_output_finale, \"training_set\", autore)\n",
    "                salva_paragrafi_separati(paragrafi_autore, cartella_training_autore, \"par\")\n",
    "\n",
    "                training_stats[autore] = len(paragrafi_autore)\n",
    "                print(f\"    Training: {len(paragrafi_autore)} paragrafi salvati\")\n",
    "            else:\n",
    "                print(f\"    Nessun paragrafo valido estratto\")\n",
    "                training_stats[autore] = 0\n",
    "        else:\n",
    "            print(f\"    File non trovato: {nome_file_autore}\")\n",
    "            training_stats[autore] = 0\n",
    "\n",
    "    # TEST & EVAL SET\n",
    "    print(f\"\\nProcessing TEST & EVAL SET:\")\n",
    "    test_eval_stats = {}\n",
    "\n",
    "    for autore in autori:\n",
    "        nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "        file_source = os.path.join(cartella_libri_puliti_test_val, nome_file_autore)\n",
    "\n",
    "        print(f\"\\n  {autore}:\")\n",
    "\n",
    "        if os.path.exists(file_source):\n",
    "            # Estrai paragrafi\n",
    "            paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                file_source,\n",
    "                min_words=min_words,\n",
    "                max_words=max_words,\n",
    "                debug=debug\n",
    "            )\n",
    "\n",
    "            if paragrafi_autore:\n",
    "                # Verifica\n",
    "                if verifica_output:\n",
    "                    verifica_paragrafi_estratti(paragrafi_autore, min_words, max_words)\n",
    "\n",
    "                # Dividi in test e eval\n",
    "                if len(paragrafi_autore) >= 2:\n",
    "                    paragrafi_test, paragrafi_eval = train_test_split(\n",
    "                        paragrafi_autore,\n",
    "                        test_size=test_size,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                else:\n",
    "                    # Se c'√® solo 1 paragrafo, mettilo nel test\n",
    "                    paragrafi_test = paragrafi_autore\n",
    "                    paragrafi_eval = []\n",
    "\n",
    "                # Salva\n",
    "                cartella_test_autore = os.path.join(cartella_output_finale, \"test_set\", autore)\n",
    "                cartella_eval_autore = os.path.join(cartella_output_finale, \"eval_set\", autore)\n",
    "\n",
    "                salva_paragrafi_separati(paragrafi_test, cartella_test_autore, \"par\")\n",
    "                salva_paragrafi_separati(paragrafi_eval, cartella_eval_autore, \"par\")\n",
    "\n",
    "                test_eval_stats[autore] = {\n",
    "                    'totale': len(paragrafi_autore),\n",
    "                    'test': len(paragrafi_test),\n",
    "                    'eval': len(paragrafi_eval)\n",
    "                }\n",
    "\n",
    "                print(f\"    Totale: {len(paragrafi_autore)} ‚Üí Test: {len(paragrafi_test)}, Eval: {len(paragrafi_eval)}\")\n",
    "            else:\n",
    "                print(f\"    Nessun paragrafo valido estratto\")\n",
    "                test_eval_stats[autore] = {'totale': 0, 'test': 0, 'eval': 0}\n",
    "        else:\n",
    "            print(f\"    File non trovato: {nome_file_autore}\")\n",
    "            test_eval_stats[autore] = {'totale': 0, 'test': 0, 'eval': 0}\n",
    "\n",
    "    # RIEPILOGO FINALE\n",
    "    print(f\"\\nRIEPILOGO DATASET FINALE:\")\n",
    "    print(f\"Parametri: parole per paragrafo {min_words}-{max_words}\")\n",
    "\n",
    "    # AGGIUNGI QUESTO messaggio se √® stato creato il corpus\n",
    "    if crea_embedding_corpus:\n",
    "        print(f\"\\n‚úì CORPUS WORD EMBEDDING creato in: word_embedding_corpus/\")\n",
    "        print(f\"  ‚Üí Usa 'corpus_completo.txt' per addestrare FastText\")\n",
    "\n",
    "    print(f\"\\nTRAINING SET:\")\n",
    "    for autore, count in training_stats.items():\n",
    "        print(f\"  {autore}: {count} paragrafi\")\n",
    "\n",
    "    print(f\"\\nTEST SET:\")\n",
    "    for autore, stats in test_eval_stats.items():\n",
    "        print(f\"  {autore}: {stats['test']} paragrafi\")\n",
    "\n",
    "    print(f\"\\nEVAL SET:\")\n",
    "    for autore, stats in test_eval_stats.items():\n",
    "        print(f\"  {autore}: {stats['eval']} paragrafi\")\n",
    "\n",
    "    print(f\"\\nTOTALE PER AUTORE:\")\n",
    "    for autore in autori:\n",
    "        training_count = training_stats.get(autore, 0)\n",
    "        test_count = test_eval_stats.get(autore, {}).get('test', 0)\n",
    "        eval_count = test_eval_stats.get(autore, {}).get('eval', 0)\n",
    "        totale = training_count + test_count + eval_count\n",
    "        print(f\"  {autore}: {totale} paragrafi totali\")\n",
    "\n",
    "    return cartella_output_finale\n",
    "\n",
    "def salva_paragrafi_separati(paragrafi, cartella_output, prefisso=\"par\"):\n",
    "    \"\"\"Salva ogni paragrafo come file separato\"\"\"\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "\n",
    "    for i, paragrafo in enumerate(paragrafi, 1):\n",
    "        nome_file = f\"{prefisso}-{i:03d}.txt\"  # Numerazione con zeri iniziali\n",
    "        percorso_file = os.path.join(cartella_output, nome_file)\n",
    "\n",
    "        with open(percorso_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(paragrafo)\n",
    "\n",
    "# TEST DELLA FUNZIONE\n",
    "if __name__ == \"__main__\":\n",
    "    # Esempio di uso\n",
    "    print(\"Test conteggio parole:\")\n",
    "\n",
    "    testi_test = [\n",
    "        \"Questa √® una frase con esattamente otto parole totali.\",\n",
    "        \"Una   frase    con     spazi      multipli.\",\n",
    "        \"Frase, con! punteggiatura? attaccata: alle; parole.\",\n",
    "        \"   Spazi iniziali e finali   \",\n",
    "        \"\",\n",
    "        \"Una sola parola\"\n",
    "    ]\n",
    "\n",
    "    for testo in testi_test:\n",
    "        parole = conta_parole_correttamente(testo)\n",
    "        print(f\"'{testo}' ‚Üí {parole} parole\")"
   ],
   "metadata": {
    "id": "Af-3qHAi5KQk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757076574352,
     "user_tz": -120,
     "elapsed": 48,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_path = crea_struttura_dataset_finale_corretta(\n",
    "    cartella_libri_puliti_training=\"/content/drive/MyDrive/libri_puliti_training\",\n",
    "    cartella_libri_puliti_test_val=\"/content/drive/MyDrive/libri_test_val_puliti\",\n",
    "    cartella_output_finale=\"/content/drive/MyDrive/dataset_authorship_finale\",\n",
    "    min_words=50,\n",
    "    max_words=100,\n",
    "    debug=True,\n",
    "    verifica_output=True,\n",
    "    crea_embedding_corpus=True\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx4YmHG05NVL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757076645163,
     "user_tz": -120,
     "elapsed": 68127,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "bbd3bf57-804e-4c00-fbd2-f6279b9bb183"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "CREAZIONE CORPUS WORD EMBEDDING (50-100 parole)\n",
      "==================================================\n",
      "\n",
      "  Processando training:\n",
      "    Paragrafo 2: 5 parole - '==================================================...'\n",
      "    Paragrafo 3: 17 parole - 'BY R. AUSTIN FREEMAN Author of ‚ÄúThe Singing Bone,‚Äù...'\n",
      "    Stats - Validi: 1827, Corti: 5976, Lunghi: 1037, Vuoti: 9\n",
      "    primo autore: 1827 paragrafi\n",
      "    Paragrafo 2: 2 parole - '==================================================...'\n",
      "    Paragrafo 3: 2 parole - 'BIG LAUREL...'\n",
      "    Stats - Validi: 2125, Corti: 14505, Lunghi: 1436, Vuoti: 19\n",
      "    secondo autore: 2125 paragrafi\n",
      "    Paragrafo 2: 3 parole - '==================================================...'\n",
      "    Paragrafo 3: 6 parole - 'INDIAN AND HUNTING STORIES FOR BOYS...'\n",
      "    Stats - Validi: 1559, Corti: 6914, Lunghi: 1057, Vuoti: 21\n",
      "    terzo autore: 1559 paragrafi\n",
      "\n",
      "  Processando test_val:\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 4 parole - 'The Eye of Osiris...'\n",
      "    Paragrafo 3: 4 parole - 'A Detective Story by...'\n",
      "    Stats - Validi: 426, Corti: 1632, Lunghi: 223, Vuoti: 0\n",
      "    primo autore: 426 paragrafi\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 7 parole - '[Frontispiece: A new tenderness swept over her]...'\n",
      "    Paragrafo 3: 2 parole - 'THE TRIFLERS...'\n",
      "    Stats - Validi: 283, Corti: 2349, Lunghi: 170, Vuoti: 1\n",
      "    secondo autore: 283 paragrafi\n",
      "    Paragrafo 1: 3 parole - 'Transcriber's Note:...'\n",
      "    Paragrafo 2: 38 parole - 'Every effort has been made to replicate this text ...'\n",
      "    Paragrafo 3: 20 parole - 'Italic text has been marked with _underscores_. Bo...'\n",
      "    Stats - Validi: 258, Corti: 1013, Lunghi: 170, Vuoti: 2\n",
      "    terzo autore: 258 paragrafi\n",
      "\n",
      "‚úì Corpus salvato: 6478 paragrafi, vocabulary: 20,323\n",
      "  ‚Üí Usa 'corpus_completo.txt' per addestrare FastText\n",
      "\n",
      "==================================================\n",
      "CREAZIONE DATASET CLASSIFICATION (50-100 parole)\n",
      "==================================================\n",
      "\n",
      "Processing TRAINING SET:\n",
      "    Paragrafo 2: 5 parole - '==================================================...'\n",
      "    Paragrafo 3: 17 parole - 'BY R. AUSTIN FREEMAN Author of ‚ÄúThe Singing Bone,‚Äù...'\n",
      "    Stats - Validi: 1827, Corti: 5976, Lunghi: 1037, Vuoti: 9\n",
      "    primo autore: 1827 paragrafi\n",
      "    Paragrafo 2: 2 parole - '==================================================...'\n",
      "    Paragrafo 3: 2 parole - 'BIG LAUREL...'\n",
      "    Stats - Validi: 2125, Corti: 14505, Lunghi: 1436, Vuoti: 19\n",
      "    secondo autore: 2125 paragrafi\n",
      "    Paragrafo 2: 3 parole - '==================================================...'\n",
      "    Paragrafo 3: 6 parole - 'INDIAN AND HUNTING STORIES FOR BOYS...'\n",
      "    Stats - Validi: 1559, Corti: 6914, Lunghi: 1057, Vuoti: 21\n",
      "    terzo autore: 1559 paragrafi\n",
      "\n",
      "Processing TEST & EVAL SET:\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 4 parole - 'The Eye of Osiris...'\n",
      "    Paragrafo 3: 4 parole - 'A Detective Story by...'\n",
      "    Stats - Validi: 426, Corti: 1632, Lunghi: 223, Vuoti: 0\n",
      "    primo autore: 426 paragrafi\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 7 parole - '[Frontispiece: A new tenderness swept over her]...'\n",
      "    Paragrafo 3: 2 parole - 'THE TRIFLERS...'\n",
      "    Stats - Validi: 283, Corti: 2349, Lunghi: 170, Vuoti: 1\n",
      "    secondo autore: 283 paragrafi\n",
      "    Paragrafo 1: 3 parole - 'Transcriber's Note:...'\n",
      "    Paragrafo 2: 38 parole - 'Every effort has been made to replicate this text ...'\n",
      "    Paragrafo 3: 20 parole - 'Italic text has been marked with _underscores_. Bo...'\n",
      "    Stats - Validi: 258, Corti: 1013, Lunghi: 170, Vuoti: 2\n",
      "    terzo autore: 258 paragrafi\n",
      "    primo autore: 426 totali ‚Üí Test: 213, Eval: 213\n",
      "    secondo autore: 283 totali ‚Üí Test: 141, Eval: 142\n",
      "    terzo autore: 258 totali ‚Üí Test: 129, Eval: 129\n",
      "\n",
      "RIEPILOGO FINALE:\n",
      "‚úì CORPUS EMBEDDING: word_embedding_corpus/corpus_completo.txt\n",
      "\n",
      "TRAINING SET:\n",
      "  primo autore: 1827 paragrafi\n",
      "  secondo autore: 2125 paragrafi\n",
      "  terzo autore: 1559 paragrafi\n",
      "\n",
      "TEST SET:\n",
      "  primo autore: 213 paragrafi\n",
      "  secondo autore: 141 paragrafi\n",
      "  terzo autore: 129 paragrafi\n",
      "\n",
      "EVAL SET:\n",
      "  primo autore: 213 paragrafi\n",
      "  secondo autore: 142 paragrafi\n",
      "  terzo autore: 129 paragrafi\n",
      "\n",
      "TOTALE PER AUTORE:\n",
      "  primo autore: 2253 paragrafi totali\n",
      "  secondo autore: 2408 paragrafi totali\n",
      "  terzo autore: 1817 paragrafi totali\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Estrazione features"
   ],
   "metadata": {
    "id": "_oliAPOEbQjQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per la generazione delle features uso prima i 3 libri del training set che sono contenuti nella cartella: \"libri_puliti\". E successivamente i file contenenti un paragrafo per il test set ed evaluation set. Dopo averli zippati li ho passati al profilatore present al link: \"http://linguistic-profiling.italianlp.it/\"."
   ],
   "metadata": {
    "id": "h9dilWHJXuyA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "id": "yLC7JmBDOxoh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1756410627225,
     "user_tz": -120,
     "elapsed": 2256,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "53abbbd5-cea6-4f69-977a-cafb18228095"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'primo'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-1733219137.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;31m# Estrai le features per ogni autore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0mprimo_autore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mautori_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'primo'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0msecondo_autore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mautori_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'secondo'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0mterzo_autore\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mautori_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'terzo'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'primo'"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_path = \"/content/drive/MyDrive/16596.csv\"\n",
    "\n",
    "def load_csv_features(csv_path):\n",
    "    \"\"\"\n",
    "    Carica le features dal CSV e le divide per autore\n",
    "    Ritorna: feature_names, primo_autore, secondo_autore, terzo_autore\n",
    "    \"\"\"\n",
    "\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Prima riga = header con nomi delle features\n",
    "    header = lines[0].strip().split('\\t')\n",
    "    feature_names = header[1:]  # Escludi 'Filename'\n",
    "\n",
    "    # Inizializza dizionario per gli autori\n",
    "    autori_features = {}\n",
    "\n",
    "    # Processa ogni riga (ogni autore)\n",
    "    for line in lines[1:]:  # Salta l'header\n",
    "        if line.strip():  # Se la riga non √® vuota\n",
    "            parts = line.strip().split('\\t')\n",
    "            filename = parts[0]\n",
    "            features = [float(val) for val in parts[1:]]  # Converti in numeri\n",
    "\n",
    "            # Identifica l'autore dal filename\n",
    "            if 'primo_autore' in filename:\n",
    "                autori_features['primo'] = features\n",
    "            elif 'secondo_autore' in filename:\n",
    "                autori_features['secondo'] = features\n",
    "            elif 'terzo_autore' in filename:\n",
    "                autori_features['terzo'] = features\n",
    "\n",
    "    return feature_names, autori_features\n",
    "\n",
    "# Carica i dati\n",
    "feature_names, autori_data = load_csv_features(csv_path)\n",
    "\n",
    "# Estrai le features per ogni autore\n",
    "primo_autore = autori_data['primo']\n",
    "secondo_autore = autori_data['secondo']\n",
    "terzo_autore = autori_data['terzo']\n",
    "\n",
    "# Stampa informazioni\n",
    "print(\"üîç ESTRAZIONE COMPLETATA\")\n",
    "print(f\"üìä Numero features totali: {len(feature_names)}\")\n",
    "print(f\"üî∏ Primo autore: {len(primo_autore)} features\")\n",
    "print(f\"üî∏ Secondo autore: {len(secondo_autore)} features\")\n",
    "print(f\"üî∏ Terzo autore: {len(terzo_autore)} features\")\n",
    "\n",
    "print(f\"\\nüìù Prime 10 features:\")\n",
    "for i, name in enumerate(feature_names[:10]):\n",
    "    print(f\"   {i+1:2d}. {name}\")\n",
    "\n",
    "print(f\"\\nüî¢ Esempio valori primo autore (prime 5 features):\")\n",
    "for i in range(5):\n",
    "    print(f\"   {feature_names[i]}: {primo_autore[i]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ VARIABILI CREATE:\")\n",
    "print(\"   ‚Ä¢ feature_names: lista con i nomi delle 142 features\")\n",
    "print(\"   ‚Ä¢ primo_autore: lista con 142 valori numerici\")\n",
    "print(\"   ‚Ä¢ secondo_autore: lista con 142 valori numerici\")\n",
    "print(\"   ‚Ä¢ terzo_autore: lista con 142 valori numerici\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Normalizzazione delle features"
   ],
   "metadata": {
    "id": "13aIZ16Yc173"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funzione di estrazione di paragrafi per il test set e validation set"
   ],
   "metadata": {
    "id": "D-d7_1WFhsHW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "def dividi_eval_test_paragrafi_ottimizzato(cartella_input):\n",
    "    \"\"\"\n",
    "    Versione OTTIMIZZATA per file singoli - molto pi√π veloce\n",
    "    \"\"\"\n",
    "\n",
    "    # Percorsi\n",
    "    cartella_output = os.path.join(cartella_input, \"libri_suddivisi\")\n",
    "    cartella_eval = os.path.join(cartella_output, \"eval_set\")\n",
    "    cartella_test = os.path.join(cartella_output, \"test_set\")\n",
    "\n",
    "    nomi_file = [\n",
    "        \"primo_autore_tutti_i_libri.txt\",\n",
    "        \"secondo_autore_tutti_i_libri.txt\",\n",
    "        \"terzo_autore_tutti_i_libri.txt\"\n",
    "    ]\n",
    "\n",
    "    nomi_autori = [\"primo_autore\", \"secondo_autore\", \"terzo_autore\"]\n",
    "\n",
    "    # 1. PULIZIA VELOCE\n",
    "    if os.path.exists(cartella_output):\n",
    "        print(\"üóëÔ∏è  Rimuovendo cartelle esistenti...\")\n",
    "        shutil.rmtree(cartella_output)\n",
    "\n",
    "    # Crea tutte le cartelle in una volta\n",
    "    for nome_autore in nomi_autori:\n",
    "        os.makedirs(os.path.join(cartella_eval, nome_autore), exist_ok=True)\n",
    "        os.makedirs(os.path.join(cartella_test, nome_autore), exist_ok=True)\n",
    "\n",
    "    print(\"‚úÖ Struttura cartelle creata\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # 2. PROCESSAMENTO OTTIMIZZATO\n",
    "    for i, nome_file in enumerate(nomi_file):\n",
    "        percorso_file = os.path.join(cartella_input, nome_file)\n",
    "        nome_autore = nomi_autori[i]\n",
    "\n",
    "        print(f\"üìñ Processando {nome_autore}...\")\n",
    "        start_time = time.time() if 'time' in globals() else None\n",
    "\n",
    "        # Leggi tutto il contenuto UNA volta\n",
    "        with open(percorso_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            contenuto = f.read()\n",
    "\n",
    "        # MIGLIORAMENTO 1: Dividi correttamente i paragrafi\n",
    "        # Usa doppi newline per paragrafi veri\n",
    "        paragrafi_grezzi = contenuto.split(\"\\n\")\n",
    "\n",
    "        # MIGLIORAMENTO 2: Filtra paragrafi validi (50-200 parole)\n",
    "        paragrafi_validi = []\n",
    "        for p in paragrafi_grezzi:\n",
    "            p = p.strip()\n",
    "            if p and 50 <= len(p.split()) <= 100:\n",
    "                # Pulisci il paragrafo\n",
    "                p = ' '.join(p.split())  # Normalizza spazi\n",
    "                paragrafi_validi.append(p)\n",
    "\n",
    "        total_paragrafi = len(paragrafi_validi)\n",
    "        punto_divisione = total_paragrafi // 2\n",
    "\n",
    "        eval_paragrafi = paragrafi_validi[:punto_divisione]\n",
    "        test_paragrafi = paragrafi_validi[punto_divisione:]\n",
    "\n",
    "        # MIGLIORAMENTO 3: Scrittura parallela dei file\n",
    "        cartella_autore_eval = os.path.join(cartella_eval, nome_autore)\n",
    "        cartella_autore_test = os.path.join(cartella_test, nome_autore)\n",
    "\n",
    "        # Usa thread per scrivere eval e test in parallelo\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            future_eval = executor.submit(\n",
    "                scrivi_paragrafi_batch, eval_paragrafi, cartella_autore_eval, \"par\"\n",
    "            )\n",
    "            future_test = executor.submit(\n",
    "                scrivi_paragrafi_batch, test_paragrafi, cartella_autore_test, \"par\"\n",
    "            )\n",
    "\n",
    "            # Aspetta completamento\n",
    "            future_eval.result()\n",
    "            future_test.result()\n",
    "\n",
    "        # Statistiche\n",
    "        if start_time:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"   ‚ö° Completato in {elapsed:.1f}s\")\n",
    "\n",
    "        print(f\"   ‚úÖ {nome_autore}:\")\n",
    "        print(f\"      ‚Ä¢ Paragrafi validi: {total_paragrafi}\")\n",
    "        print(f\"      ‚Ä¢ Eval: {len(eval_paragrafi)} file\")\n",
    "        print(f\"      ‚Ä¢ Test: {len(test_paragrafi)} file\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(\"‚úÖ COMPLETATO!\")\n",
    "    print(f\"üìÅ File salvati in: {cartella_output}\")\n",
    "\n",
    "\n",
    "def scrivi_paragrafi_batch(paragrafi, cartella_destinazione, prefisso):\n",
    "    \"\"\"\n",
    "    Scrive paragrafi come file singoli in modo ottimizzato\n",
    "    \"\"\"\n",
    "\n",
    "    # MIGLIORAMENTO 4: Batch di operazioni I/O\n",
    "    buffer_size = 50  # Scrivi 50 file alla volta\n",
    "\n",
    "    for i in range(0, len(paragrafi), buffer_size):\n",
    "        batch = paragrafi[i:i + buffer_size]\n",
    "\n",
    "        # Scrivi batch di file\n",
    "        for j, paragrafo in enumerate(batch):\n",
    "            idx_globale = i + j + 1\n",
    "            nome_file = f\"{prefisso}-{idx_globale}.txt\"\n",
    "            percorso_file = os.path.join(cartella_destinazione, nome_file)\n",
    "\n",
    "            # MIGLIORAMENTO 5: Scrittura ottimizzata\n",
    "            with open(percorso_file, \"w\", encoding=\"utf-8\", buffering=8192) as f:\n",
    "                f.write(paragrafo)\n",
    "\n",
    "\n",
    "def conta_file_esistenti(cartella_input):\n",
    "    \"\"\"\n",
    "    Conta quanti file verranno creati (per stima tempo)\n",
    "    \"\"\"\n",
    "    nomi_file = [\n",
    "        \"primo_autore_tutti_i_libri.txt\",\n",
    "        \"secondo_autore_tutti_i_libri.txt\",\n",
    "        \"terzo_autore_tutti_i_libri.txt\"\n",
    "    ]\n",
    "\n",
    "    total_paragrafi = 0\n",
    "\n",
    "    for nome_file in nomi_file:\n",
    "        percorso_file = os.path.join(cartella_input, nome_file)\n",
    "        if os.path.exists(percorso_file):\n",
    "            with open(percorso_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                contenuto = f.read()\n",
    "\n",
    "            paragrafi = [p.strip() for p in contenuto.split(\"\\n\\n\")\n",
    "                        if p.strip() and 50 <= len(p.split()) <= 200]\n",
    "\n",
    "            total_paragrafi += len(paragrafi)\n",
    "            print(f\"üìä {nome_file}: ~{len(paragrafi)} paragrafi validi\")\n",
    "\n",
    "    print(f\"üìä TOTALE STIMATO: ~{total_paragrafi} paragrafi\")\n",
    "    print(f\"üìä FILE DA CREARE: ~{total_paragrafi * 2} (eval + test)\")\n",
    "\n",
    "    if total_paragrafi > 5000:\n",
    "        print(\"‚ö†Ô∏è  ATTENZIONE: Molti file! Tempo stimato: 2-5 minuti\")\n",
    "    elif total_paragrafi > 1000:\n",
    "        print(\"‚ÑπÔ∏è  Tempo stimato: 30-60 secondi\")\n",
    "    else:\n",
    "        print(\"‚úÖ Tempo stimato: <30 secondi\")\n",
    "\n",
    "\n",
    "# UTILIZZO:\n",
    "import time\n",
    "\n",
    "print(\"üìä ANALISI PRELIMINARE:\")\n",
    "conta_file_esistenti(\"/content/drive/MyDrive/libri_test_val_puliti\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ AVVIO DIVISIONE OTTIMIZZATA:\")\n",
    "dividi_eval_test_paragrafi_ottimizzato(\"/content/drive/MyDrive/libri_test_val_puliti\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "V0xO0psehzaR",
    "executionInfo": {
     "status": "error",
     "timestamp": 1756043787669,
     "user_tz": -120,
     "elapsed": 177687,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "c539b9b7-3145-4753-e981-524f806c3d40"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üóëÔ∏è  Cartelle pulite e ricreate\n",
      "--------------------------------------------------\n",
      "üìñ PRIMO AUTORE:\n",
      "   ‚Ä¢ Paragrafi totali: 8797\n",
      "   ‚Ä¢ Eval set: 4398 paragrafi ‚Üí /content/drive/MyDrive/libri_test_val_puliti/libri_suddivisi/eval_set/primo autore\n",
      "   ‚Ä¢ Test set: 4399 paragrafi ‚Üí /content/drive/MyDrive/libri_test_val_puliti/libri_suddivisi/test_set/primo autore\n",
      "--------------------------------------------------\n",
      "üìñ SECONDO AUTORE:\n",
      "   ‚Ä¢ Paragrafi totali: 6965\n",
      "   ‚Ä¢ Eval set: 3482 paragrafi ‚Üí /content/drive/MyDrive/libri_test_val_puliti/libri_suddivisi/eval_set/secondo autore\n",
      "   ‚Ä¢ Test set: 3483 paragrafi ‚Üí /content/drive/MyDrive/libri_test_val_puliti/libri_suddivisi/test_set/secondo autore\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-2854380229.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;31m# Esempio di utilizzo:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 86\u001B[0;31m \u001B[0mdividi_eval_test_paragrafi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/content/drive/MyDrive/libri_test_val_puliti\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipython-input-2854380229.py\u001B[0m in \u001B[0;36mdividi_eval_test_paragrafi\u001B[0;34m(cartella_input)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mpercorso_par\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcartella_autore_eval\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnome_par_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpercorso_par\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"w\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"utf-8\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparagrafo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Successivamente a questa ultima estrazione ho passato al ProfilingUD i dati e li ho scaricati ed inseriti nella cartella libri_suddivisi/sets profiati"
   ],
   "metadata": {
    "id": "m747w8zLTmbT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estrazione dati e normalizzazione"
   ],
   "metadata": {
    "id": "w_vhXiO5Vfq7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_paragrafi_features_fixed(csv_path):\n",
    "    \"\"\"\n",
    "    Carica features da CSV con struttura mista:\n",
    "    - Training: file unici per autore (corpus completo)\n",
    "    - Test/Eval: paragrafi individuali\n",
    "    \"\"\"\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header = lines[0].strip().split('\\t')\n",
    "    feature_names = header[1:]\n",
    "\n",
    "    # Inizializza dizionari\n",
    "    primo_autore_training = {}\n",
    "    secondo_autore_training = {}\n",
    "    terzo_autore_training = {}\n",
    "\n",
    "    primo_autore_data_test = {}\n",
    "    secondo_autore_data_test = {}\n",
    "    terzo_autore_data_test = {}\n",
    "\n",
    "    primo_autore_data_eval = {}\n",
    "    secondo_autore_data_eval = {}\n",
    "    terzo_autore_data_eval = {}\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "\n",
    "        filename = parts[0]\n",
    "        try:\n",
    "            features = [float(val) for val in parts[1:]]\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # Training (corpus completi)\n",
    "        if 'train_set' in filename_lower:\n",
    "            file_key = filename.split('/')[-1].replace('.conllu', '')\n",
    "            if 'primo' in filename_lower:\n",
    "                primo_autore_training[file_key] = features\n",
    "            elif 'secondo' in filename_lower:\n",
    "                secondo_autore_training[file_key] = features\n",
    "            elif 'terzo' in filename_lower:\n",
    "                terzo_autore_training[file_key] = features\n",
    "\n",
    "        # Test (paragrafi individuali)\n",
    "        elif 'test_set' in filename_lower:\n",
    "            par_name = filename.split('/')[-1].replace('.conllu', '')\n",
    "            if 'primo autore' in filename_lower:\n",
    "                primo_autore_data_test[par_name] = features\n",
    "            elif 'secondo autore' in filename_lower:\n",
    "                secondo_autore_data_test[par_name] = features\n",
    "            elif 'terzo autore' in filename_lower:\n",
    "                terzo_autore_data_test[par_name] = features\n",
    "\n",
    "        # Eval (paragrafi individuali)\n",
    "        elif 'eval_set' in filename_lower:\n",
    "            par_name = filename.split('/')[-1].replace('.conllu', '')\n",
    "            if 'primo autore' in filename_lower:\n",
    "                primo_autore_data_eval[par_name] = features\n",
    "            elif 'secondo autore' in filename_lower:\n",
    "                secondo_autore_data_eval[par_name] = features\n",
    "            elif 'terzo autore' in filename_lower:\n",
    "                terzo_autore_data_eval[par_name] = features\n",
    "\n",
    "    # Costruisci tuple\n",
    "    libri_train = (primo_autore_training, secondo_autore_training, terzo_autore_training)\n",
    "    libri_test = (primo_autore_data_test, secondo_autore_data_test, terzo_autore_data_test)\n",
    "    libri_eval = (primo_autore_data_eval, secondo_autore_data_eval, terzo_autore_data_eval)\n",
    "\n",
    "    return feature_names, libri_test, libri_eval, libri_train\n",
    "\n",
    "# Carica i dati\n",
    "nomi_features, libri_test_data, libri_eval_data, libri_train = load_paragrafi_features_fixed(\"/content/drive/MyDrive/dati profilati completo/csvConProfilingUnico.csv\")\n",
    "\n",
    "# Mostra sample dei dati caricati\n",
    "print(\"üìä SAMPLE DATI CARICATI:\")\n",
    "\n",
    "print(f\"\\nüéØ TRAINING ({sum(len(d) for d in libri_train)} elementi):\")\n",
    "for i, autore_dict in enumerate(libri_train):\n",
    "    if autore_dict:\n",
    "        sample_key = list(autore_dict.keys())[0]\n",
    "        print(f\"  Autore {i+1}: {sample_key} -> {len(autore_dict[sample_key])} features\")\n",
    "\n",
    "\n",
    "print(f\"\\nüìñ TEST ({sum(len(d) for d in libri_test_data)} paragrafi):\")\n",
    "for i, autore_dict in enumerate(libri_test_data):\n",
    "    if autore_dict:\n",
    "        sample_keys = list(autore_dict.keys())[:3]\n",
    "        print(f\"  Autore {i+1}: {len(autore_dict)} paragrafi (es: {sample_keys})\")\n",
    "\n",
    "print(f\"\\nüìù EVAL ({sum(len(d) for d in libri_eval_data)} paragrafi):\")\n",
    "for i, autore_dict in enumerate(libri_eval_data):\n",
    "    if autore_dict:\n",
    "        sample_keys = list(autore_dict.keys())[:3]\n",
    "        print(f\"  Autore {i+1}: {len(autore_dict)} paragrafi (es: {sample_keys})\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"üìä SAMPLE DATI CARICATI:\\n\")\n",
    "print(\"cacca\")\n",
    "# TRAINING\n",
    "print(f\"üéØ TRAINING ({sum(len(d) for d in libri_train)} libri):\")\n",
    "for i, autore_dict in enumerate(libri_train):\n",
    "    print(f\"  Autore {i+1}: {len(autore_dict)} libri\")\n",
    "    for libro, feat in autore_dict.items():\n",
    "        print(f\"    - {libro}: {len(feat)} features\")\n",
    "    print()\n",
    "\n",
    "# TEST\n",
    "print(f\"üìñ TEST ({sum(len(d) for d in libri_test_data)} paragrafi):\")\n",
    "for i, autore_dict in enumerate(libri_test_data):\n",
    "    print(f\"  Autore {i+1}: {len(autore_dict)} paragrafi\")\n",
    "    for par, feat in list(autore_dict.items())[:5]:  # mostro solo i primi 5 per non intasare\n",
    "        print(f\"    - {par}: {len(feat)} features\")\n",
    "    print()\n",
    "\n",
    "# EVAL\n",
    "print(f\"üìù EVAL ({sum(len(d) for d in libri_eval_data)} paragrafi):\")\n",
    "for i, autore_dict in enumerate(libri_eval_data):\n",
    "    print(f\"  Autore {i+1}: {len(autore_dict)} paragrafi\")\n",
    "    for par, feat in list(autore_dict.items())[:5]:  # mostro solo i primi 5\n",
    "        print(f\"    - {par}: {len(feat)} features\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "gas_-66zY-uT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def test_normality_and_scale(X_train, libri_test_data, libri_eval_data, libri_train):\n",
    "    \"\"\"\n",
    "    Testa normalit√† e applica scaling con la nuova struttura mista:\n",
    "    - Training: corpus completi per autore\n",
    "    - Test/Eval: paragrafi individuali per autore\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "    normal_features = 0\n",
    "\n",
    "    print(\"üß™ TEST NORMALIT√Ä (campione di 10 features):\")\n",
    "\n",
    "    # Test normalit√† su prime 10 features\n",
    "    for i in range(min(10, n_features)):\n",
    "        feature_data = X_train[:, i]\n",
    "        statistic, p_value = stats.shapiro(feature_data)\n",
    "        is_normal = p_value > 0.05\n",
    "        if is_normal:\n",
    "            normal_features += 1\n",
    "        print(f\"   Feature {i+1}: {'‚úì Normale' if is_normal else '‚úó Non normale'} (p={p_value:.4f})\")\n",
    "\n",
    "    # Scelta scaler\n",
    "    normality_ratio = normal_features / min(10, n_features)\n",
    "    if normality_ratio >= 0.6:\n",
    "        print(\"   üéØ SCELTA: StandardScaler\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler_name = \"StandardScaler\"\n",
    "    else:\n",
    "        print(\"   üéØ SCELTA: MinMaxScaler\")\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler_name = \"MinMaxScaler\"\n",
    "\n",
    "    # Applica normalizzazione al training\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Processa training dal CSV (corpus completi)\n",
    "    print(f\"\\nüîß TRAINING DAL CSV ({len(libri_train)} autori):\")\n",
    "    train_csv_scaled = []\n",
    "    for i, autore_dict in enumerate(libri_train):\n",
    "        if len(autore_dict) > 0:\n",
    "            autore_features = np.array(list(autore_dict.values()))\n",
    "            if autore_features.shape[1] == n_features:\n",
    "                autore_scaled = scaler.transform(autore_features)\n",
    "                train_csv_scaled.append(autore_scaled)\n",
    "                print(f\"   Autore {i+1}: {autore_features.shape} ‚Üí normalizzato\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Autore {i+1}: mismatch ({autore_features.shape[1]} vs {n_features} features)\")\n",
    "                train_csv_scaled.append(np.array([]))\n",
    "        else:\n",
    "            train_csv_scaled.append(np.array([]))\n",
    "\n",
    "    # Processa test set (paragrafi individuali)\n",
    "    print(f\"\\nüîß TEST SET ({len(libri_test_data)} autori):\")\n",
    "    test_scaled_data = []\n",
    "    for i, autore_dict in enumerate(libri_test_data):\n",
    "        if len(autore_dict) > 0:\n",
    "            autore_features = np.array(list(autore_dict.values()))\n",
    "            if autore_features.shape[1] == n_features:\n",
    "                autore_scaled = scaler.transform(autore_features)\n",
    "                test_scaled_data.append(autore_scaled)\n",
    "                print(f\"   Autore {i+1}: {autore_features.shape} ‚Üí normalizzato\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Autore {i+1}: mismatch ({autore_features.shape[1]} vs {n_features})\")\n",
    "                test_scaled_data.append(np.array([]))\n",
    "        else:\n",
    "            test_scaled_data.append(np.array([]))\n",
    "\n",
    "    # Processa eval set (paragrafi individuali)\n",
    "    print(f\"\\nüîß EVAL SET ({len(libri_eval_data)} autori):\")\n",
    "    eval_scaled_data = []\n",
    "    for i, autore_dict in enumerate(libri_eval_data):\n",
    "        if len(autore_dict) > 0:\n",
    "            autore_features = np.array(list(autore_dict.values()))\n",
    "            if autore_features.shape[1] == n_features:\n",
    "                autore_scaled = scaler.transform(autore_features)\n",
    "                eval_scaled_data.append(autore_scaled)\n",
    "                print(f\"   Autore {i+1}: {autore_features.shape} ‚Üí normalizzato\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Autore {i+1}: mismatch ({autore_features.shape[1]} vs {n_features})\")\n",
    "                eval_scaled_data.append(np.array([]))\n",
    "        else:\n",
    "            eval_scaled_data.append(np.array([]))\n",
    "\n",
    "    print(f\"\\n‚úÖ NORMALIZZAZIONE COMPLETATA ({scaler_name})\")\n",
    "    return scaler, X_train_scaled, test_scaled_data, eval_scaled_data, train_csv_scaled\n",
    "\n",
    "def stampa_esempi_dataset(X_train_scaled, test_scaled_data, eval_scaled_data, train_csv_scaled=None):\n",
    "    \"\"\"\n",
    "    Stampa esempi dei dataset normalizzati\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã ESEMPI DATI NORMALIZZATI\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Training originale\n",
    "    print(f\"\\nüèãÔ∏è TRAINING SET:\")\n",
    "    print(f\"   Shape: {X_train_scaled.shape}\")\n",
    "    print(f\"   Prime 2 righe, prime 5 features:\")\n",
    "    for i in range(min(2, X_train_scaled.shape[0])):\n",
    "        values = [f\"{val:7.3f}\" for val in X_train_scaled[i, :5]]\n",
    "        print(f\"     [{', '.join(values)}...]\")\n",
    "\n",
    "    # Training dal CSV (se disponibile)\n",
    "    if train_csv_scaled:\n",
    "        print(f\"\\nüèãÔ∏è TRAINING DAL CSV:\")\n",
    "        for i, autore_data in enumerate(train_csv_scaled):\n",
    "            if autore_data.size > 0:\n",
    "                print(f\"   Autore {i+1}: {autore_data.shape}\")\n",
    "                values = [f\"{val:7.3f}\" for val in autore_data[0, :5]]\n",
    "                print(f\"     [{', '.join(values)}...]\")\n",
    "\n",
    "    # Test set\n",
    "    print(f\"\\nüß™ TEST SET:\")\n",
    "    for i, autore_data in enumerate(test_scaled_data):\n",
    "        if autore_data.size > 0:\n",
    "            print(f\"   Autore {i+1}: {autore_data.shape}\")\n",
    "            print(f\"   Prime 2 righe:\")\n",
    "            for j in range(min(2, autore_data.shape[0])):\n",
    "                values = [f\"{val:7.3f}\" for val in autore_data[j, :5]]\n",
    "                print(f\"     [{', '.join(values)}...]\")\n",
    "        else:\n",
    "            print(f\"   Autore {i+1}: Vuoto\")\n",
    "\n",
    "    # Eval set\n",
    "    print(f\"\\nüéØ EVAL SET:\")\n",
    "    for i, autore_data in enumerate(eval_scaled_data):\n",
    "        if autore_data.size > 0:\n",
    "            print(f\"   Autore {i+1}: {autore_data.shape}\")\n",
    "            print(f\"   Prime 2 righe:\")\n",
    "            for j in range(min(2, autore_data.shape[0])):\n",
    "                values = [f\"{val:7.3f}\" for val in autore_data[j, :5]]\n",
    "                print(f\"     [{', '.join(values)}...]\")\n",
    "        else:\n",
    "            print(f\"   Autore {i+1}: Vuoto\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Esegui normalizzazione\n",
    "# X_train = unione di tutti i libri completi\n",
    "X_train = np.vstack([np.array(list(d.values())) for d in libri_train if len(d) > 0])\n",
    "\n",
    "# Esegui normalizzazione\n",
    "scaler, X_train_scaled, test_scaled_data, eval_scaled_data, train_csv_scaled = test_normality_and_scale(\n",
    "    X_train, libri_test_data, libri_eval_data, libri_train\n",
    ")\n",
    "\n",
    "\n",
    "# Stampa esempi\n",
    "stampa_esempi_dataset(X_train_scaled, test_scaled_data, eval_scaled_data, train_csv_scaled)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "CAwjEVqZZk8_",
    "executionInfo": {
     "status": "error",
     "timestamp": 1756405667162,
     "user_tz": -120,
     "elapsed": 172,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "249d7fa7-1e9e-498c-bceb-d114c996efcf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'libri_train' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-3270829535.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[0;31m# Esegui normalizzazione\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[0;31m# X_train = unione di tutti i libri completi\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 148\u001B[0;31m \u001B[0mX_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlibri_train\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;31m# Esegui normalizzazione\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'libri_train' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "def load_csv_unified(csv_path):\n",
    "    \"\"\"\n",
    "    Carica il CSV unico e divide i dati per set (training/test/eval) e autore\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Caricamento CSV: {csv_path}\")\n",
    "\n",
    "    # Leggi CSV\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Header\n",
    "    header = lines[0].strip().split('\\t')\n",
    "    feature_names = header[1:]  # Escludi 'Filename'\n",
    "    print(f\"üìä Features trovate: {len(feature_names)}\")\n",
    "\n",
    "    # Inizializza strutture dati\n",
    "    data = {\n",
    "        'training_set': {'primo_autore': [], 'secondo_autore': [], 'terzo_autore': []},\n",
    "        'test_set': {'primo_autore': [], 'secondo_autore': [], 'terzo_autore': []},\n",
    "        'eval_set': {'primo_autore': [], 'secondo_autore': [], 'terzo_autore': []}\n",
    "    }\n",
    "\n",
    "    # Processa ogni riga\n",
    "    for line_num, line in enumerate(lines[1:], 1):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "\n",
    "        filename = parts[0]\n",
    "        try:\n",
    "            features = [float(val) for val in parts[1:]]\n",
    "        except ValueError:\n",
    "            print(f\"‚ö†Ô∏è  Errore conversione riga {line_num}: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Identifica set e autore dal path\n",
    "        if 'training_set' in filename:\n",
    "            set_type = 'training_set'\n",
    "        elif 'test_set' in filename:\n",
    "            set_type = 'test_set'\n",
    "        elif 'eval_set' in filename:\n",
    "            set_type = 'eval_set'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Identifica autore\n",
    "        if 'primo autore' in filename:\n",
    "            autore = 'primo_autore'\n",
    "        elif 'secondo autore' in filename:\n",
    "            autore = 'secondo_autore'\n",
    "        elif 'terzo autore' in filename:\n",
    "            autore = 'terzo_autore'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Aggiungi i dati\n",
    "        data[set_type][autore].append(features)\n",
    "\n",
    "    # Converti in numpy array\n",
    "    for set_type in data:\n",
    "        for autore in data[set_type]:\n",
    "            if data[set_type][autore]:\n",
    "                data[set_type][autore] = np.array(data[set_type][autore])\n",
    "            else:\n",
    "                data[set_type][autore] = np.array([]).reshape(0, len(feature_names))\n",
    "\n",
    "    # Stampa statistiche\n",
    "    print(\"\\nüìä DATI CARICATI:\")\n",
    "    for set_type in ['training_set', 'test_set', 'eval_set']:\n",
    "        print(f\"  {set_type.upper()}:\")\n",
    "        for autore in ['primo_autore', 'secondo_autore', 'terzo_autore']:\n",
    "            shape = data[set_type][autore].shape\n",
    "            print(f\"    {autore}: {shape[0]} paragrafi x {shape[1]} features\")\n",
    "\n",
    "    return feature_names, data\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    \"\"\"\n",
    "    Prepara i dati di training unificati (tutti gli autori insieme)\n",
    "    \"\"\"\n",
    "    print(\"\\nüèãÔ∏è PREPARAZIONE TRAINING DATA:\")\n",
    "\n",
    "    training_data = data['training_set']\n",
    "\n",
    "    # Unisci tutti i dati di training\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i, autore in enumerate(['primo_autore', 'secondo_autore', 'terzo_autore']):\n",
    "        autore_data = training_data[autore]\n",
    "        if autore_data.shape[0] > 0:\n",
    "            X_train.append(autore_data)\n",
    "            y_train.extend([i] * autore_data.shape[0])  # Label: 0, 1, 2\n",
    "            print(f\"  {autore}: {autore_data.shape[0]} campioni ‚Üí label {i}\")\n",
    "\n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    print(f\"  üìà Training finale: {X_train.shape} (X) + {y_train.shape} (y)\")\n",
    "    print(f\"  üìä Distribuzione classi: {np.bincount(y_train)}\")\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def prepare_test_eval_data(data):\n",
    "    \"\"\"\n",
    "    Prepara i dati di test e eval\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ PREPARAZIONE TEST & EVAL DATA:\")\n",
    "\n",
    "    # Test set\n",
    "    test_data = data['test_set']\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i, autore in enumerate(['primo_autore', 'secondo_autore', 'terzo_autore']):\n",
    "        autore_data = test_data[autore]\n",
    "        if autore_data.shape[0] > 0:\n",
    "            X_test.append(autore_data)\n",
    "            y_test.extend([i] * autore_data.shape[0])\n",
    "            print(f\"  TEST {autore}: {autore_data.shape[0]} campioni\")\n",
    "\n",
    "    X_test = np.vstack(X_test) if X_test else np.array([]).reshape(0, len(data['training_set']['primo_autore'][0]) if data['training_set']['primo_autore'].shape[0] > 0 else 0)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Eval set\n",
    "    eval_data = data['eval_set']\n",
    "    X_eval = []\n",
    "    y_eval = []\n",
    "\n",
    "    for i, autore in enumerate(['primo_autore', 'secondo_autore', 'terzo_autore']):\n",
    "        autore_data = eval_data[autore]\n",
    "        if autore_data.shape[0] > 0:\n",
    "            X_eval.append(autore_data)\n",
    "            y_eval.extend([i] * autore_data.shape[0])\n",
    "            print(f\"  EVAL {autore}: {autore_data.shape[0]} campioni\")\n",
    "\n",
    "    X_eval = np.vstack(X_eval) if X_eval else np.array([]).reshape(0, len(data['training_set']['primo_autore'][0]) if data['training_set']['primo_autore'].shape[0] > 0 else 0)\n",
    "    y_eval = np.array(y_eval)\n",
    "\n",
    "    print(f\"  üìà Test finale: {X_test.shape} (X) + {y_test.shape} (y)\")\n",
    "    print(f\"  üìà Eval finale: {X_eval.shape} (X) + {y_eval.shape} (y)\")\n",
    "\n",
    "    return X_test, y_test, X_eval, y_eval\n",
    "\n",
    "def apply_scaling(X_train, X_test, X_eval, method='auto'):\n",
    "    \"\"\"\n",
    "    Applica scaling ai dati\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß APPLICAZIONE SCALING (metodo: {method}):\")\n",
    "\n",
    "    # Scelta automatica del metodo\n",
    "    if method == 'auto':\n",
    "        # Test normalit√† su campione di features\n",
    "        normal_count = 0\n",
    "        sample_size = min(10, X_train.shape[1])\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            _, p_value = stats.shapiro(X_train[:, i])\n",
    "            if p_value > 0.05:\n",
    "                normal_count += 1\n",
    "\n",
    "        normality_ratio = normal_count / sample_size\n",
    "        if normality_ratio >= 0.6:\n",
    "            scaler = StandardScaler()\n",
    "            scaler_name = \"StandardScaler\"\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler_name = \"MinMaxScaler\"\n",
    "\n",
    "        print(f\"  üéØ Scelta automatica: {scaler_name} (normalit√†: {normality_ratio:.2f})\")\n",
    "\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        scaler_name = \"StandardScaler\"\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler_name = \"MinMaxScaler\"\n",
    "\n",
    "    # Applica scaling\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test) if X_test.shape[0] > 0 else X_test\n",
    "    X_eval_scaled = scaler.transform(X_eval) if X_eval.shape[0] > 0 else X_eval\n",
    "\n",
    "    print(f\"  ‚úÖ Scaling applicato:\")\n",
    "    print(f\"    Training: {X_train_scaled.shape}\")\n",
    "    print(f\"    Test: {X_test_scaled.shape}\")\n",
    "    print(f\"    Eval: {X_eval_scaled.shape}\")\n",
    "\n",
    "    return scaler, X_train_scaled, X_test_scaled, X_eval_scaled\n",
    "\n",
    "def show_data_sample(feature_names, X_train, y_train, X_test, y_test, X_eval, y_eval, n_samples=3):\n",
    "    \"\"\"\n",
    "    Mostra un campione dei dati caricati\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã CAMPIONE DATI (primi {n_samples} esempi per set):\")\n",
    "\n",
    "    # Training\n",
    "    print(f\"\\nüèãÔ∏è TRAINING SET:\")\n",
    "    for i in range(min(n_samples, X_train.shape[0])):\n",
    "        values = [f\"{val:6.3f}\" for val in X_train[i, :5]]  # Prime 5 features\n",
    "        print(f\"  Campione {i+1} (autore {y_train[i]}): [{', '.join(values)}...]\")\n",
    "\n",
    "    # Test\n",
    "    if X_test.shape[0] > 0:\n",
    "        print(f\"\\nüß™ TEST SET:\")\n",
    "        for i in range(min(n_samples, X_test.shape[0])):\n",
    "            values = [f\"{val:6.3f}\" for val in X_test[i, :5]]\n",
    "            print(f\"  Campione {i+1} (autore {y_test[i]}): [{', '.join(values)}...]\")\n",
    "\n",
    "    # Eval\n",
    "    if X_eval.shape[0] > 0:\n",
    "        print(f\"\\nüéØ EVAL SET:\")\n",
    "        for i in range(min(n_samples, X_eval.shape[0])):\n",
    "            values = [f\"{val:6.3f}\" for val in X_eval[i, :5]]\n",
    "            print(f\"  Campione {i+1} (autore {y_eval[i]}): [{', '.join(values)}...]\")\n",
    "\n",
    "    print(f\"\\nüìä FEATURE NAMES (prime 10):\")\n",
    "    for i, name in enumerate(feature_names[:10]):\n",
    "        print(f\"  {i+1:2d}. {name}\")\n",
    "\n",
    "def get_data_summary(feature_names, data):\n",
    "    \"\"\"\n",
    "    Stampa un riepilogo completo dei dati\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RIEPILOGO COMPLETO DATASET\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"üìã Features totali: {len(feature_names)}\")\n",
    "\n",
    "    total_samples = 0\n",
    "    for set_type in ['training_set', 'test_set', 'eval_set']:\n",
    "        set_total = 0\n",
    "        print(f\"\\nüìÇ {set_type.upper()}:\")\n",
    "\n",
    "        for autore in ['primo_autore', 'secondo_autore', 'terzo_autore']:\n",
    "            count = data[set_type][autore].shape[0]\n",
    "            set_total += count\n",
    "            print(f\"  {autore}: {count:,} paragrafi\")\n",
    "\n",
    "        print(f\"  üìä Totale {set_type}: {set_total:,} paragrafi\")\n",
    "        total_samples += set_total\n",
    "\n",
    "    print(f\"\\nüéØ TOTALE GENERALE: {total_samples:,} paragrafi\")\n",
    "    print(\"=\"*60)"
   ],
   "metadata": {
    "id": "i6pg3qZ0PfT-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CHIAMATE DELLE FUNZIONI - PROCESSAMENTO DATASET COMPLETO\n",
    "# =============================================================================\n",
    "\n",
    "# Percorso del CSV unico con tutte le features\n",
    "csv_path = \"/content/drive/MyDrive/16596.csv\"\n",
    "\n",
    "print(\"üöÄ INIZIO PROCESSAMENTO DATASET COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. CARICAMENTO DATI DAL CSV\n",
    "print(\"STEP 1: Caricamento dati dal CSV...\")\n",
    "feature_names, data = load_csv_unified(csv_path)\n",
    "\n",
    "# 2. RIEPILOGO INIZIALE\n",
    "get_data_summary(feature_names, data)\n",
    "\n",
    "# 3. PREPARAZIONE TRAINING DATA\n",
    "print(\"\\nSTEP 2: Preparazione training data...\")\n",
    "X_train, y_train = prepare_training_data(data)\n",
    "\n",
    "# 4. PREPARAZIONE TEST & EVAL DATA\n",
    "print(\"\\nSTEP 3: Preparazione test & eval data...\")\n",
    "X_test, y_test, X_eval, y_eval = prepare_test_eval_data(data)\n",
    "\n",
    "# 5. APPLICAZIONE SCALING\n",
    "print(\"\\nSTEP 4: Applicazione scaling...\")\n",
    "scaler, X_train_scaled, X_test_scaled, X_eval_scaled = apply_scaling(\n",
    "    X_train, X_test, X_eval, method='auto'\n",
    ")\n",
    "\n",
    "# 6. MOSTRA CAMPIONE DATI (prima dello scaling)\n",
    "print(\"\\nSTEP 5: Campione dati originali...\")\n",
    "show_data_sample(feature_names, X_train, y_train, X_test, y_test, X_eval, y_eval)\n",
    "\n",
    "# 7. MOSTRA CAMPIONE DATI SCALATI\n",
    "print(\"\\nSTEP 6: Campione dati scalati...\")\n",
    "show_data_sample(feature_names, X_train_scaled, y_train, X_test_scaled, y_test, X_eval_scaled, y_eval)\n",
    "\n",
    "# 8. RIEPILOGO FINALE\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PROCESSAMENTO COMPLETATO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä VARIABILI DISPONIBILI:\")\n",
    "print(f\"  ‚Ä¢ feature_names: {len(feature_names)} nomi features\")\n",
    "print(f\"  ‚Ä¢ X_train_scaled: {X_train_scaled.shape} (training features)\")\n",
    "print(f\"  ‚Ä¢ y_train: {y_train.shape} (training labels)\")\n",
    "print(f\"  ‚Ä¢ X_test_scaled: {X_test_scaled.shape} (test features)\")\n",
    "print(f\"  ‚Ä¢ y_test: {y_test.shape} (test labels)\")\n",
    "print(f\"  ‚Ä¢ X_eval_scaled: {X_eval_scaled.shape} (eval features)\")\n",
    "print(f\"  ‚Ä¢ y_eval: {y_eval.shape} (eval labels)\")\n",
    "print(f\"  ‚Ä¢ scaler: {type(scaler).__name__} (per nuovi dati)\")\n",
    "print(f\"  ‚Ä¢ data: dizionario con dati originali per autore/set\")\n",
    "\n",
    "print(\"\\nüéØ LABELS MAPPING:\")\n",
    "print(\"  0 ‚Üí primo_autore\")\n",
    "print(\"  1 ‚Üí secondo_autore\")\n",
    "print(\"  2 ‚Üí terzo_autore\")\n",
    "\n",
    "print(\"\\nüî• PRONTO PER IL TRAINING!\")\n",
    "print(\"=\" * 60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzYNJhw5Pk5Y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756414657687,
     "user_tz": -120,
     "elapsed": 974,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "3daf5f6f-3265-4feb-bd25-f3522bd13a02"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ INIZIO PROCESSAMENTO DATASET COMPLETO\n",
      "============================================================\n",
      "STEP 1: Caricamento dati dal CSV...\n",
      "üìÇ Caricamento CSV: /content/drive/MyDrive/16596.csv\n",
      "üìä Features trovate: 127\n",
      "\n",
      "üìä DATI CARICATI:\n",
      "  TRAINING_SET:\n",
      "    primo_autore: 1841 paragrafi x 127 features\n",
      "    secondo_autore: 2125 paragrafi x 127 features\n",
      "    terzo_autore: 1559 paragrafi x 127 features\n",
      "  TEST_SET:\n",
      "    primo_autore: 213 paragrafi x 127 features\n",
      "    secondo_autore: 0 paragrafi x 127 features\n",
      "    terzo_autore: 0 paragrafi x 127 features\n",
      "  EVAL_SET:\n",
      "    primo_autore: 8 paragrafi x 127 features\n",
      "    secondo_autore: 0 paragrafi x 127 features\n",
      "    terzo_autore: 0 paragrafi x 127 features\n",
      "\n",
      "============================================================\n",
      "üìä RIEPILOGO COMPLETO DATASET\n",
      "============================================================\n",
      "üìã Features totali: 127\n",
      "\n",
      "üìÇ TRAINING_SET:\n",
      "  primo_autore: 1,841 paragrafi\n",
      "  secondo_autore: 2,125 paragrafi\n",
      "  terzo_autore: 1,559 paragrafi\n",
      "  üìä Totale training_set: 5,525 paragrafi\n",
      "\n",
      "üìÇ TEST_SET:\n",
      "  primo_autore: 213 paragrafi\n",
      "  secondo_autore: 0 paragrafi\n",
      "  terzo_autore: 0 paragrafi\n",
      "  üìä Totale test_set: 213 paragrafi\n",
      "\n",
      "üìÇ EVAL_SET:\n",
      "  primo_autore: 8 paragrafi\n",
      "  secondo_autore: 0 paragrafi\n",
      "  terzo_autore: 0 paragrafi\n",
      "  üìä Totale eval_set: 8 paragrafi\n",
      "\n",
      "üéØ TOTALE GENERALE: 5,746 paragrafi\n",
      "============================================================\n",
      "\n",
      "STEP 2: Preparazione training data...\n",
      "\n",
      "üèãÔ∏è PREPARAZIONE TRAINING DATA:\n",
      "  primo_autore: 1841 campioni ‚Üí label 0\n",
      "  secondo_autore: 2125 campioni ‚Üí label 1\n",
      "  terzo_autore: 1559 campioni ‚Üí label 2\n",
      "  üìà Training finale: (5525, 127) (X) + (5525,) (y)\n",
      "  üìä Distribuzione classi: [1841 2125 1559]\n",
      "\n",
      "STEP 3: Preparazione test & eval data...\n",
      "\n",
      "üß™ PREPARAZIONE TEST & EVAL DATA:\n",
      "  TEST primo_autore: 213 campioni\n",
      "  EVAL primo_autore: 8 campioni\n",
      "  üìà Test finale: (213, 127) (X) + (213,) (y)\n",
      "  üìà Eval finale: (8, 127) (X) + (8,) (y)\n",
      "\n",
      "STEP 4: Applicazione scaling...\n",
      "\n",
      "üîß APPLICAZIONE SCALING (metodo: auto):\n",
      "  üéØ Scelta automatica: MinMaxScaler (normalit√†: 0.00)\n",
      "  ‚úÖ Scaling applicato:\n",
      "    Training: (5525, 127)\n",
      "    Test: (213, 127)\n",
      "    Eval: (8, 127)\n",
      "\n",
      "STEP 5: Campione dati originali...\n",
      "\n",
      "üìã CAMPIONE DATI (primi 3 esempi per set):\n",
      "\n",
      "üèãÔ∏è TRAINING SET:\n",
      "  Campione 1 (autore 0): [ 3.000, 71.000, 23.667,  4.625,  0.000...]\n",
      "  Campione 2 (autore 0): [ 4.000, 93.000, 23.250,  4.518,  0.000...]\n",
      "  Campione 3 (autore 0): [ 5.000, 99.000, 19.800,  4.311,  0.000...]\n",
      "\n",
      "üß™ TEST SET:\n",
      "  Campione 1 (autore 0): [ 2.000, 56.000, 28.000,  4.196,  0.000...]\n",
      "  Campione 2 (autore 0): [ 4.000, 112.000, 28.000,  4.545,  0.000...]\n",
      "  Campione 3 (autore 0): [ 5.000, 107.000, 21.400,  4.462,  0.000...]\n",
      "\n",
      "üéØ EVAL SET:\n",
      "  Campione 1 (autore 0): [ 4.000, 72.000, 18.000,  4.377,  0.000...]\n",
      "  Campione 2 (autore 0): [ 4.000, 79.000, 19.750,  4.015,  0.000...]\n",
      "  Campione 3 (autore 0): [ 3.000, 108.000, 36.000,  4.546,  0.000...]\n",
      "\n",
      "üìä FEATURE NAMES (prime 10):\n",
      "   1. n_sentences\n",
      "   2. n_tokens\n",
      "   3. tokens_per_sent\n",
      "   4. char_per_tok\n",
      "   5. ttr_lemma_chunks_100\n",
      "   6. ttr_form_chunks_100\n",
      "   7. upos_dist_ADJ\n",
      "   8. upos_dist_ADP\n",
      "   9. upos_dist_ADV\n",
      "  10. upos_dist_AUX\n",
      "\n",
      "STEP 6: Campione dati scalati...\n",
      "\n",
      "üìã CAMPIONE DATI (primi 3 esempi per set):\n",
      "\n",
      "üèãÔ∏è TRAINING SET:\n",
      "  Campione 1 (autore 0): [ 0.067,  0.162,  0.191,  0.561,  0.000...]\n",
      "  Campione 2 (autore 0): [ 0.100,  0.360,  0.187,  0.526,  0.000...]\n",
      "  Campione 3 (autore 0): [ 0.133,  0.414,  0.152,  0.459,  0.000...]\n",
      "\n",
      "üß™ TEST SET:\n",
      "  Campione 1 (autore 0): [ 0.033,  0.027,  0.234,  0.422,  0.000...]\n",
      "  Campione 2 (autore 0): [ 0.100,  0.532,  0.234,  0.535,  0.000...]\n",
      "  Campione 3 (autore 0): [ 0.133,  0.486,  0.168,  0.508,  0.000...]\n",
      "\n",
      "üéØ EVAL SET:\n",
      "  Campione 1 (autore 0): [ 0.100,  0.171,  0.134,  0.480,  0.000...]\n",
      "  Campione 2 (autore 0): [ 0.100,  0.234,  0.152,  0.363,  0.000...]\n",
      "  Campione 3 (autore 0): [ 0.067,  0.495,  0.313,  0.535,  0.000...]\n",
      "\n",
      "üìä FEATURE NAMES (prime 10):\n",
      "   1. n_sentences\n",
      "   2. n_tokens\n",
      "   3. tokens_per_sent\n",
      "   4. char_per_tok\n",
      "   5. ttr_lemma_chunks_100\n",
      "   6. ttr_form_chunks_100\n",
      "   7. upos_dist_ADJ\n",
      "   8. upos_dist_ADP\n",
      "   9. upos_dist_ADV\n",
      "  10. upos_dist_AUX\n",
      "\n",
      "============================================================\n",
      "‚úÖ PROCESSAMENTO COMPLETATO!\n",
      "============================================================\n",
      "üìä VARIABILI DISPONIBILI:\n",
      "  ‚Ä¢ feature_names: 127 nomi features\n",
      "  ‚Ä¢ X_train_scaled: (5525, 127) (training features)\n",
      "  ‚Ä¢ y_train: (5525,) (training labels)\n",
      "  ‚Ä¢ X_test_scaled: (213, 127) (test features)\n",
      "  ‚Ä¢ y_test: (213,) (test labels)\n",
      "  ‚Ä¢ X_eval_scaled: (8, 127) (eval features)\n",
      "  ‚Ä¢ y_eval: (8,) (eval labels)\n",
      "  ‚Ä¢ scaler: MinMaxScaler (per nuovi dati)\n",
      "  ‚Ä¢ data: dizionario con dati originali per autore/set\n",
      "\n",
      "üéØ LABELS MAPPING:\n",
      "  0 ‚Üí primo_autore\n",
      "  1 ‚Üí secondo_autore\n",
      "  2 ‚Üí terzo_autore\n",
      "\n",
      "üî• PRONTO PER IL TRAINING!\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/scipy/stats/_axis_nan_policy.py:579: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 5525.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_svm_authorship(X_train_scaled, y_train, feature_names):\n",
    "    \"\"\"\n",
    "    Addestra il modello SVM per authorship attribution\n",
    "    \"\"\"\n",
    "    print(\"üöÄ TRAINING SVM AUTHORSHIP CLASSIFIER\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Configurazione SVM\n",
    "    # dual=False quando n_samples > n_features (raccomandato per dataset grandi)\n",
    "    svm_model = LinearSVC(\n",
    "        dual=False,           # False perch√© abbiamo molti samples\n",
    "        max_iter=10000,       # Aumentiamo iterazioni per convergenza\n",
    "        random_state=42,      # Per riproducibilit√†\n",
    "        class_weight='balanced'  # Bilancia classi se necessario\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Dati training: {X_train_scaled.shape}\")\n",
    "    print(f\"üìã Distribuzione classi: {np.bincount(y_train)}\")\n",
    "    print(f\"üîß Configurazione SVM: dual=False, max_iter=10000, balanced\")\n",
    "\n",
    "    # Training\n",
    "    print(\"\\nüèãÔ∏è Training del modello...\")\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"‚úÖ Training completato!\")\n",
    "    return svm_model\n",
    "\n",
    "def cross_validation_5fold(X_train_scaled, y_train, svm_model=None):\n",
    "    \"\"\"\n",
    "    Esegue cross-validation a 5 fold sul training set\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ CROSS-VALIDATION 5-FOLD\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Configurazione cross-validation\n",
    "    cv_splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    if svm_model is None:\n",
    "        svm_model = LinearSVC(dual=False, max_iter=10000, random_state=42, class_weight='balanced')\n",
    "\n",
    "    # Metriche da valutare\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1_macro': 'f1_macro',\n",
    "        'f1_micro': 'f1_micro',\n",
    "        'f1_weighted': 'f1_weighted'\n",
    "    }\n",
    "\n",
    "    print(\"üìä Esecuzione 5-fold cross-validation...\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results = cross_validate(\n",
    "        svm_model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        cv=cv_splitter,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Baseline per confronto\n",
    "    baseline = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "    baseline_scores = cross_validate(\n",
    "        baseline,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        cv=cv_splitter,\n",
    "        scoring={'accuracy': 'accuracy', 'f1_macro': 'f1_macro'}\n",
    "    )\n",
    "\n",
    "    # Stampa risultati\n",
    "    print(\"\\nüìà RISULTATI CROSS-VALIDATION:\")\n",
    "    print(f\"  üéØ Accuracy     : {cv_results['test_accuracy'].mean():.4f} ¬± {cv_results['test_accuracy'].std():.4f}\")\n",
    "    print(f\"  üìä F1-Macro     : {cv_results['test_f1_macro'].mean():.4f} ¬± {cv_results['test_f1_macro'].std():.4f}\")\n",
    "    print(f\"  üìä F1-Micro     : {cv_results['test_f1_micro'].mean():.4f} ¬± {cv_results['test_f1_micro'].std():.4f}\")\n",
    "    print(f\"  üìä F1-Weighted  : {cv_results['test_f1_weighted'].mean():.4f} ¬± {cv_results['test_f1_weighted'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\nüìâ BASELINE (Most Frequent):\")\n",
    "    print(f\"  üéØ Accuracy     : {baseline_scores['test_accuracy'].mean():.4f} ¬± {baseline_scores['test_accuracy'].std():.4f}\")\n",
    "    print(f\"  üìä F1-Macro     : {baseline_scores['test_f1_macro'].mean():.4f} ¬± {baseline_scores['test_f1_macro'].std():.4f}\")\n",
    "\n",
    "    print(f\"\\n‚ú® MIGLIORAMENTO vs BASELINE:\")\n",
    "    acc_improvement = cv_results['test_accuracy'].mean() - baseline_scores['test_accuracy'].mean()\n",
    "    f1_improvement = cv_results['test_f1_macro'].mean() - baseline_scores['test_f1_macro'].mean()\n",
    "    print(f\"  üöÄ Accuracy: +{acc_improvement:.4f}\")\n",
    "    print(f\"  üöÄ F1-Macro: +{f1_improvement:.4f}\")\n",
    "\n",
    "    # Dettaglio per fold\n",
    "    print(f\"\\nüìã DETTAGLIO PER FOLD:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  Fold {i+1}: Acc={cv_results['test_accuracy'][i]:.4f}, F1={cv_results['test_f1_macro'][i]:.4f}\")\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "def evaluate_on_test_eval(svm_model, X_test_scaled, y_test, X_eval_scaled, y_eval,\n",
    "                         author_names=['primo_autore', 'secondo_autore', 'terzo_autore']):\n",
    "    \"\"\"\n",
    "    Valuta il modello sui set di test e validation (eval)\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ VALUTAZIONE SU TEST E EVAL SET\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # --- TEST SET ---\n",
    "    if X_test_scaled.shape[0] > 0:\n",
    "        print(f\"\\nüìä TEST SET ({X_test_scaled.shape[0]} campioni):\")\n",
    "\n",
    "        # Predizioni\n",
    "        test_predictions = svm_model.predict(X_test_scaled)\n",
    "        test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "        test_f1_macro = f1_score(y_test, test_predictions, average='macro')\n",
    "        test_f1_weighted = f1_score(y_test, test_predictions, average='weighted')\n",
    "\n",
    "        print(f\"  üéØ Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"  üìä F1-Macro: {test_f1_macro:.4f}\")\n",
    "        print(f\"  üìä F1-Weighted: {test_f1_weighted:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        print(f\"\\nüìã CLASSIFICATION REPORT (Test):\")\n",
    "        test_report = classification_report(\n",
    "            y_test,\n",
    "            test_predictions,\n",
    "            target_names=author_names,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(test_report)\n",
    "\n",
    "        results['test'] = {\n",
    "            'accuracy': test_accuracy,\n",
    "            'f1_macro': test_f1_macro,\n",
    "            'f1_weighted': test_f1_weighted,\n",
    "            'predictions': test_predictions,\n",
    "            'report': test_report\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  TEST SET VUOTO - SKIP\")\n",
    "        results['test'] = None\n",
    "\n",
    "    # --- EVAL SET ---\n",
    "    if X_eval_scaled.shape[0] > 0:\n",
    "        print(f\"\\nüìä EVAL SET ({X_eval_scaled.shape[0]} campioni):\")\n",
    "\n",
    "        # Predizioni\n",
    "        eval_predictions = svm_model.predict(X_eval_scaled)\n",
    "        eval_accuracy = accuracy_score(y_eval, eval_predictions)\n",
    "        eval_f1_macro = f1_score(y_eval, eval_predictions, average='macro')\n",
    "        eval_f1_weighted = f1_score(y_eval, eval_predictions, average='weighted')\n",
    "\n",
    "        print(f\"  üéØ Accuracy: {eval_accuracy:.4f}\")\n",
    "        print(f\"  üìä F1-Macro: {eval_f1_macro:.4f}\")\n",
    "        print(f\"  üìä F1-Weighted: {eval_f1_weighted:.4f}\")\n",
    "\n",
    "        # Classification report\n",
    "        print(f\"\\nüìã CLASSIFICATION REPORT (Eval):\")\n",
    "        eval_report = classification_report(\n",
    "            y_eval,\n",
    "            eval_predictions,\n",
    "            target_names=author_names,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(eval_report)\n",
    "\n",
    "        results['eval'] = {\n",
    "            'accuracy': eval_accuracy,\n",
    "            'f1_macro': eval_f1_macro,\n",
    "            'f1_weighted': eval_f1_weighted,\n",
    "            'predictions': eval_predictions,\n",
    "            'report': eval_report\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  EVAL SET VUOTO - SKIP\")\n",
    "        results['eval'] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_confusion_matrices(svm_model, X_test_scaled, y_test, X_eval_scaled, y_eval,\n",
    "                           author_names=['primo_autore', 'secondo_autore', 'terzo_autore']):\n",
    "    \"\"\"\n",
    "    Mostra le confusion matrix per test e eval set\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä CONFUSION MATRICES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Test set\n",
    "    if X_test_scaled.shape[0] > 0:\n",
    "        test_predictions = svm_model.predict(X_test_scaled)\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test,\n",
    "            test_predictions,\n",
    "            display_labels=author_names,\n",
    "            ax=axes[0],\n",
    "            cmap='Blues'\n",
    "        )\n",
    "        axes[0].set_title('Test Set Confusion Matrix')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, 'Test Set Vuoto', ha='center', va='center', transform=axes[0].transAxes)\n",
    "        axes[0].set_title('Test Set - Non Disponibile')\n",
    "\n",
    "    # Eval set\n",
    "    if X_eval_scaled.shape[0] > 0:\n",
    "        eval_predictions = svm_model.predict(X_eval_scaled)\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_eval,\n",
    "            eval_predictions,\n",
    "            display_labels=author_names,\n",
    "            ax=axes[1],\n",
    "            cmap='Greens'\n",
    "        )\n",
    "        axes[1].set_title('Eval Set Confusion Matrix')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Eval Set Vuoto', ha='center', va='center', transform=axes[1].transAxes)\n",
    "        axes[1].set_title('Eval Set - Non Disponibile')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_feature_importance(svm_model, feature_names, author_names=['primo_autore', 'secondo_autore', 'terzo_autore'], top_n=20):\n",
    "    \"\"\"\n",
    "    Analizza l'importanza delle features per ogni autore\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç FEATURE IMPORTANCE ANALYSIS (Top {top_n})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Coefficienti del modello SVM\n",
    "    coefs = svm_model.coef_  # Shape: (n_classes, n_features)\n",
    "\n",
    "    # Per ogni autore (classe)\n",
    "    for idx, author in enumerate(author_names):\n",
    "        print(f\"\\nüìä {author.upper()}:\")\n",
    "\n",
    "        # Coefficienti per questo autore\n",
    "        class_coefs = coefs[idx]\n",
    "\n",
    "        # Crea dizionario feature -> coefficiente\n",
    "        feature_importance = dict(zip(feature_names, class_coefs))\n",
    "\n",
    "        # Ordina per valore assoluto (pi√π importanti)\n",
    "        sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "        # Mostra top N\n",
    "        print(f\"  Top {top_n} features pi√π importanti:\")\n",
    "        for i, (feature, coef) in enumerate(sorted_features[:top_n], 1):\n",
    "            direction = \"+\" if coef > 0 else \"-\"\n",
    "            print(f\"    {i:2d}. {feature:<25} {direction} {abs(coef):.4f}\")\n",
    "\n",
    "    return coefs\n",
    "\n",
    "def summary_results(cv_results, test_eval_results):\n",
    "    \"\"\"\n",
    "    Riepilogo finale di tutti i risultati\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä RIEPILOGO FINALE RISULTATI\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Cross-validation\n",
    "    print(f\"üîÑ CROSS-VALIDATION (5-fold):\")\n",
    "    print(f\"  üéØ Accuracy: {cv_results['test_accuracy'].mean():.4f} ¬± {cv_results['test_accuracy'].std():.4f}\")\n",
    "    print(f\"  üìä F1-Macro: {cv_results['test_f1_macro'].mean():.4f} ¬± {cv_results['test_f1_macro'].std():.4f}\")\n",
    "\n",
    "    # Test set\n",
    "    if test_eval_results['test'] is not None:\n",
    "        print(f\"\\nüß™ TEST SET:\")\n",
    "        print(f\"  üéØ Accuracy: {test_eval_results['test']['accuracy']:.4f}\")\n",
    "        print(f\"  üìä F1-Macro: {test_eval_results['test']['f1_macro']:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nüß™ TEST SET: Non disponibile\")\n",
    "\n",
    "    # Eval set\n",
    "    if test_eval_results['eval'] is not None:\n",
    "        print(f\"\\nüéØ EVAL SET:\")\n",
    "        print(f\"  üéØ Accuracy: {test_eval_results['eval']['accuracy']:.4f}\")\n",
    "        print(f\"  üìä F1-Macro: {test_eval_results['eval']['f1_macro']:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nüéØ EVAL SET: Non disponibile\")\n",
    "\n",
    "    print(\"\\n‚úÖ ANALISI COMPLETATA!\")\n",
    "    print(\"=\" * 60)"
   ],
   "metadata": {
    "id": "PbX39XcpUIvd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#n grammi"
   ],
   "metadata": {
    "id": "ga3hhpAjWTq6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FUNZIONI PER SVM N-GRAMMI AUTHORSHIP ATTRIBUTION - PARTE 1\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSI PER LA STRUTTURA DATI\n",
    "# ============================================================================\n",
    "\n",
    "class Token:\n",
    "    \"\"\"Rappresenta un singolo token con attributi linguistici\"\"\"\n",
    "    def __init__(self, word, lemma, pos, upos=None):\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos  # POS fine-grained\n",
    "        self.upos = upos  # POS coarse-grained\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        return len(self.word)\n",
    "\n",
    "class Sentence:\n",
    "    \"\"\"Rappresenta una frase come sequenza di token\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        self.tokens.append(token)\n",
    "\n",
    "    def get_words(self):\n",
    "        return [token.word for token in self.tokens]\n",
    "\n",
    "    def get_lemmas(self):\n",
    "        return [token.lemma for token in self.tokens]\n",
    "\n",
    "    def get_pos(self):\n",
    "        return [token.pos for token in self.tokens]\n",
    "\n",
    "    def get_upos(self):\n",
    "        return [token.upos for token in self.tokens if token.upos]\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        num_chars = 0\n",
    "        for token in self.tokens:\n",
    "            num_chars += token.get_num_chars()\n",
    "        # Aggiungiamo spazi tra parole\n",
    "        if self.get_num_tokens() > 1:\n",
    "            num_chars += self.get_num_tokens() - 1\n",
    "        return num_chars\n",
    "\n",
    "    def __str__(self):\n",
    "        return ' '.join([token.word for token in self.tokens])\n",
    "\n",
    "class Document:\n",
    "    \"\"\"Rappresenta un documento (tutti i post di un autore)\"\"\"\n",
    "    def __init__(self, doc_id, author):\n",
    "        self.doc_id = doc_id\n",
    "        self.author = author\n",
    "        self.sentences = []\n",
    "        self.features = None\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        return sum(sentence.get_num_tokens() for sentence in self.sentences)\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        return sum(sentence.get_num_chars() for sentence in self.sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# CARICAMENTO DATI DA CONLL-U\n",
    "# ============================================================================\n",
    "\n",
    "def parse_conllu_line(line):\n",
    "    \"\"\"Estrae token da riga CoNLL-U\"\"\"\n",
    "    if not line.strip() or line.startswith('#') or not line[0].isdigit():\n",
    "        return None\n",
    "\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) < 4:\n",
    "        return None\n",
    "\n",
    "    # Salta token con indici composti (es. 5-6)\n",
    "    if '-' in fields[0]:\n",
    "        return None\n",
    "\n",
    "    word = fields[1] if len(fields) > 1 else \"\"\n",
    "    lemma = fields[2] if len(fields) > 2 else \"\"\n",
    "    upos = fields[3] if len(fields) > 3 else \"\"\n",
    "    pos = fields[4] if len(fields) > 4 else upos\n",
    "\n",
    "    return Token(word, lemma, pos, upos)\n",
    "\n",
    "def load_conllu_file(file_path):\n",
    "    \"\"\"Carica un file CoNLL-U e restituisce lista di frasi\"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = Sentence()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "\n",
    "                # Fine frase (riga vuota)\n",
    "                if not line:\n",
    "                    if current_sentence.get_num_tokens() > 0:\n",
    "                        sentences.append(current_sentence)\n",
    "                        current_sentence = Sentence()\n",
    "                    continue\n",
    "\n",
    "                # Parse token\n",
    "                token = parse_conllu_line(line)\n",
    "                if token:\n",
    "                    current_sentence.add_token(token)\n",
    "\n",
    "            # Aggiungi ultima frase se non vuota\n",
    "            if current_sentence.get_num_tokens() > 0:\n",
    "                sentences.append(current_sentence)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel caricamento {file_path}: {e}\")\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def load_dataset_from_structure(base_path):\n",
    "    \"\"\"\n",
    "    Carica dataset dalla struttura di cartelle:\n",
    "    base_path/\n",
    "      ‚îú‚îÄ‚îÄ training_set/\n",
    "      ‚îÇ   ‚îú‚îÄ‚îÄ primo_autore/\n",
    "      ‚îÇ   ‚îú‚îÄ‚îÄ secondo_autore/\n",
    "      ‚îÇ   ‚îî‚îÄ‚îÄ terzo_autore/\n",
    "      ‚îú‚îÄ‚îÄ test_set/\n",
    "      ‚îî‚îÄ‚îÄ eval_set/\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = {}\n",
    "    splits = ['training_set', 'test_set', 'eval_set']\n",
    "    authors = ['primo autore', 'secondo autore', 'terzo autore']\n",
    "\n",
    "    for split in splits:\n",
    "        documents = []\n",
    "        split_path = os.path.join(base_path, split)\n",
    "\n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"‚ö†Ô∏è  Cartella non trovata: {split_path}\")\n",
    "            documents = []\n",
    "            datasets[split] = documents\n",
    "            continue\n",
    "\n",
    "        print(f\"üìÇ Caricamento {split}...\")\n",
    "\n",
    "        for author in authors:\n",
    "            author_path = os.path.join(split_path, author)\n",
    "\n",
    "            if not os.path.exists(author_path):\n",
    "                print(f\"‚ö†Ô∏è  Cartella autore non trovata: {author_path}\")\n",
    "                continue\n",
    "\n",
    "            # Carica tutti i file dell'autore\n",
    "            doc_count = 0\n",
    "            for filename in os.listdir(author_path):\n",
    "                if filename.endswith('.conllu') or filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(author_path, filename)\n",
    "\n",
    "                    # Carica frasi dal file\n",
    "                    sentences = load_conllu_file(file_path)\n",
    "\n",
    "                    if sentences:\n",
    "                        # Crea documento\n",
    "                        doc_id = f\"{author}_{filename}\"\n",
    "                        document = Document(doc_id, author)\n",
    "\n",
    "                        for sentence in sentences:\n",
    "                            document.add_sentence(sentence)\n",
    "\n",
    "                        documents.append(document)\n",
    "                        doc_count += 1\n",
    "\n",
    "            print(f\"  üìÑ {author}: {doc_count} documenti caricati\")\n",
    "\n",
    "        datasets[split] = documents\n",
    "        print(f\"  ‚úÖ {split}: {len(documents)} documenti totali\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# ============================================================================\n",
    "# ESTRAZIONE N-GRAMMI\n",
    "# ============================================================================\n",
    "\n",
    "def extract_char_ngrams(text, n, prefix=\"CHAR\"):\n",
    "    \"\"\"Estrae n-grammi di caratteri da testo\"\"\"\n",
    "    ngrams = {}\n",
    "    text = text.lower()  # Normalizzazione\n",
    "\n",
    "    for i in range(len(text) - n + 1):\n",
    "        ngram = text[i:i+n]\n",
    "        ngram_key = f\"{prefix}_{n}_{ngram}\"\n",
    "        ngrams[ngram_key] = ngrams.get(ngram_key, 0) + 1\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def extract_word_ngrams(words, n, prefix=\"WORD\"):\n",
    "    \"\"\"Estrae n-grammi di parole\"\"\"\n",
    "    ngrams = {}\n",
    "\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = '_'.join(words[i:i+n])\n",
    "        ngram_key = f\"{prefix}_{n}_{ngram}\"\n",
    "        ngrams[ngram_key] = ngrams.get(ngram_key, 0) + 1\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def extract_document_ngrams(document, config):\n",
    "    \"\"\"\n",
    "    Estrae n-grammi da documento secondo configurazione\n",
    "\n",
    "    config = {\n",
    "        'char_ngrams': [1, 2, 3],\n",
    "        'word_ngrams': [1, 2],\n",
    "        'lemma_ngrams': [1, 2],\n",
    "        'pos_ngrams': [1, 2, 3],\n",
    "        'normalize': True\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    all_ngrams = {}\n",
    "\n",
    "    # Raccogli tutto il testo\n",
    "    all_text = \"\"\n",
    "    all_words = []\n",
    "    all_lemmas = []\n",
    "    all_pos = []\n",
    "    all_upos = []\n",
    "\n",
    "    for sentence in document.sentences:\n",
    "        sentence_text = str(sentence)\n",
    "        all_text += sentence_text + \" \"\n",
    "        all_words.extend(sentence.get_words())\n",
    "        all_lemmas.extend(sentence.get_lemmas())\n",
    "        all_pos.extend(sentence.get_pos())\n",
    "        all_upos.extend(sentence.get_upos())\n",
    "\n",
    "    # N-grammi di caratteri\n",
    "    if 'char_ngrams' in config:\n",
    "        for n in config['char_ngrams']:\n",
    "            char_ngrams = extract_char_ngrams(all_text, n)\n",
    "            all_ngrams.update(char_ngrams)\n",
    "\n",
    "    # N-grammi di parole\n",
    "    if 'word_ngrams' in config:\n",
    "        for n in config['word_ngrams']:\n",
    "            word_ngrams = extract_word_ngrams(all_words, n, \"WORD\")\n",
    "            all_ngrams.update(word_ngrams)\n",
    "\n",
    "    # N-grammi di lemmi\n",
    "    if 'lemma_ngrams' in config:\n",
    "        for n in config['lemma_ngrams']:\n",
    "            lemma_ngrams = extract_word_ngrams(all_lemmas, n, \"LEMMA\")\n",
    "            all_ngrams.update(lemma_ngrams)\n",
    "\n",
    "    # N-grammi di POS\n",
    "    if 'pos_ngrams' in config:\n",
    "        for n in config['pos_ngrams']:\n",
    "            pos_ngrams = extract_word_ngrams(all_pos, n, \"POS\")\n",
    "            all_ngrams.update(pos_ngrams)\n",
    "\n",
    "    # N-grammi di UPOS\n",
    "    if 'upos_ngrams' in config:\n",
    "        for n in config['upos_ngrams']:\n",
    "            upos_ngrams = extract_word_ngrams(all_upos, n, \"UPOS\")\n",
    "            all_ngrams.update(upos_ngrams)\n",
    "\n",
    "    # Normalizzazione\n",
    "    if config.get('normalize', True):\n",
    "        doc_length = len(all_words) if all_words else 1\n",
    "        for key in all_ngrams:\n",
    "            all_ngrams[key] = all_ngrams[key] / doc_length\n",
    "\n",
    "    return all_ngrams"
   ],
   "metadata": {
    "id": "klhjZUg4WWTu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FUNZIONI PER SVM N-GRAMMI AUTHORSHIP ATTRIBUTION - PARTE 2\n",
    "# ============================================================================\n",
    "\n",
    "def filter_rare_features(train_features_list, min_docs=2):\n",
    "    \"\"\"Filtra feature che appaiono in meno di min_docs documenti\"\"\"\n",
    "\n",
    "    # Conta in quanti documenti appare ogni feature\n",
    "    feature_doc_count = defaultdict(int)\n",
    "    for doc_features in train_features_list:\n",
    "        for feature in doc_features.keys():\n",
    "            feature_doc_count[feature] += 1\n",
    "\n",
    "    # Rimuovi feature rare dai documenti di train\n",
    "    filtered_features = []\n",
    "    for doc_features in train_features_list:\n",
    "        filtered_doc = {\n",
    "            feature: count for feature, count in doc_features.items()\n",
    "            if feature_doc_count[feature] >= min_docs\n",
    "        }\n",
    "        filtered_features.append(filtered_doc)\n",
    "\n",
    "    num_before = len(feature_doc_count)\n",
    "    num_after = len([f for f in feature_doc_count.values() if f >= min_docs])\n",
    "\n",
    "    print(f\"  üîΩ Feature filtrate: {num_before} ‚Üí {num_after} (min_docs={min_docs})\")\n",
    "\n",
    "    return filtered_features\n",
    "\n",
    "def prepare_data_for_training(documents, config):\n",
    "    \"\"\"Prepara dati per training: estrae features e crea matrici\"\"\"\n",
    "\n",
    "    print(f\"üîß Estrazione features con configurazione: {config['name']}\")\n",
    "\n",
    "    # Estrai features\n",
    "    features_list = []\n",
    "    labels = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_features = extract_document_ngrams(doc, config)\n",
    "        features_list.append(doc_features)\n",
    "        labels.append(doc.author)\n",
    "\n",
    "    # Filtra feature rare\n",
    "    if config.get('min_docs', 2) > 1:\n",
    "        features_list = filter_rare_features(features_list, config['min_docs'])\n",
    "\n",
    "    # Converti in matrice con DictVectorizer\n",
    "    vectorizer = DictVectorizer()\n",
    "    X = vectorizer.fit_transform(features_list)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Normalizzazione\n",
    "    scaler = MaxAbsScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    print(f\"  üìä Matrice features: {X_scaled.shape}\")\n",
    "    print(f\"  üë• Distribuzione classi: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    return X_scaled, y, vectorizer, scaler\n",
    "\n",
    "def prepare_test_data(documents, vectorizer, scaler, config):\n",
    "    \"\"\"Prepara dati di test usando vectorizer e scaler gi√† fittati\"\"\"\n",
    "\n",
    "    features_list = []\n",
    "    labels = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_features = extract_document_ngrams(doc, config)\n",
    "        features_list.append(doc_features)\n",
    "        labels.append(doc.author)\n",
    "\n",
    "    X = vectorizer.transform(features_list)\n",
    "    y = np.array(labels)\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURAZIONI SPERIMENTALI\n",
    "# ============================================================================\n",
    "\n",
    "def get_experimental_configurations():\n",
    "    \"\"\"Definisce tutte le configurazioni da testare\"\"\"\n",
    "\n",
    "    configs = []\n",
    "\n",
    "    # 1. Solo caratteri (diverse lunghezze)\n",
    "    configs.append({\n",
    "        'name': 'Char_1-3',\n",
    "        'char_ngrams': [1, 2, 3],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 2. Solo parole (diverse lunghezze)\n",
    "    configs.append({\n",
    "        'name': 'Word_1-2',\n",
    "        'word_ngrams': [1, 2],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 3. Solo lemmi\n",
    "    configs.append({\n",
    "        'name': 'Lemma_1-2',\n",
    "        'lemma_ngrams': [1, 2],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 4. Solo POS tags\n",
    "    configs.append({\n",
    "        'name': 'POS_1-3',\n",
    "        'pos_ngrams': [1, 2, 3],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 5. Solo UPOS tags\n",
    "    configs.append({\n",
    "        'name': 'UPOS_1-3',\n",
    "        'upos_ngrams': [1, 2, 3],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 6. Combinazione parole + caratteri\n",
    "    configs.append({\n",
    "        'name': 'Word_Char_Combined',\n",
    "        'word_ngrams': [1, 2],\n",
    "        'char_ngrams': [2, 3],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 7. Combinazione lemmi + POS\n",
    "    configs.append({\n",
    "        'name': 'Lemma_POS_Combined',\n",
    "        'lemma_ngrams': [1, 2],\n",
    "        'pos_ngrams': [1, 2, 3],\n",
    "        'normalize': True,\n",
    "        'min_docs': 2\n",
    "    })\n",
    "\n",
    "    # 8. Tutto insieme\n",
    "    configs.append({\n",
    "        'name': 'All_Features',\n",
    "        'char_ngrams': [1, 2, 3],\n",
    "        'word_ngrams': [1, 2],\n",
    "        'lemma_ngrams': [1, 2],\n",
    "        'pos_ngrams': [1, 2, 3],\n",
    "        'upos_ngrams': [1, 2],\n",
    "        'normalize': True,\n",
    "        'min_docs': 3\n",
    "    })\n",
    "\n",
    "    return configs\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING E VALUTAZIONE\n",
    "# ============================================================================\n",
    "\n",
    "def train_svm_model(X_train, y_train):\n",
    "    \"\"\"Addestra modello SVM lineare\"\"\"\n",
    "\n",
    "    svm_model = LinearSVC(\n",
    "        dual=False,  # Raccomandato quando n_samples > n_features\n",
    "        max_iter=10000,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'  # Per bilanciare classi\n",
    "    )\n",
    "\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    return svm_model\n",
    "\n",
    "def cross_validate_config(X_train, y_train, config_name, cv_folds=5):\n",
    "    \"\"\"Cross-validation per una configurazione\"\"\"\n",
    "\n",
    "    # Modello SVM\n",
    "    svm_model = LinearSVC(\n",
    "        dual=False,\n",
    "        max_iter=10000,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_splitter = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    scoring = ['accuracy', 'f1_macro', 'f1_weighted']\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        svm_model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv_splitter,\n",
    "        scoring=scoring,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    # Baseline per confronto\n",
    "    baseline = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "    baseline_results = cross_validate(\n",
    "        baseline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv_splitter,\n",
    "        scoring=['accuracy', 'f1_macro']\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'cv_accuracy_mean': cv_results['test_accuracy'].mean(),\n",
    "        'cv_accuracy_std': cv_results['test_accuracy'].std(),\n",
    "        'cv_f1_macro_mean': cv_results['test_f1_macro'].mean(),\n",
    "        'cv_f1_macro_std': cv_results['test_f1_macro'].std(),\n",
    "        'cv_f1_weighted_mean': cv_results['test_f1_weighted'].mean(),\n",
    "        'cv_f1_weighted_std': cv_results['test_f1_weighted'].std(),\n",
    "        'baseline_accuracy': baseline_results['test_accuracy'].mean(),\n",
    "        'baseline_f1_macro': baseline_results['test_f1_macro'].mean(),\n",
    "        'improvement_accuracy': cv_results['test_accuracy'].mean() - baseline_results['test_accuracy'].mean(),\n",
    "        'improvement_f1_macro': cv_results['test_f1_macro'].mean() - baseline_results['test_f1_macro'].mean()\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_on_test_set(svm_model, X_test, y_test, X_eval, y_eval, config_name):\n",
    "    \"\"\"Valuta modello su test e eval set\"\"\"\n",
    "\n",
    "    results = {'config_name': config_name}\n",
    "\n",
    "    # Test set\n",
    "    if X_test.shape[0] > 0:\n",
    "        test_pred = svm_model.predict(X_test)\n",
    "        results['test_accuracy'] = accuracy_score(y_test, test_pred)\n",
    "        results['test_f1_macro'] = f1_score(y_test, test_pred, average='macro')\n",
    "        results['test_f1_weighted'] = f1_score(y_test, test_pred, average='weighted')\n",
    "    else:\n",
    "        results['test_accuracy'] = 0\n",
    "        results['test_f1_macro'] = 0\n",
    "        results['test_f1_weighted'] = 0\n",
    "\n",
    "    # Eval set\n",
    "    if X_eval.shape[0] > 0:\n",
    "        eval_pred = svm_model.predict(X_eval)\n",
    "        results['eval_accuracy'] = accuracy_score(y_eval, eval_pred)\n",
    "        results['eval_f1_macro'] = f1_score(y_eval, eval_pred, average='macro')\n",
    "        results['eval_f1_weighted'] = f1_score(y_eval, eval_pred, average='weighted')\n",
    "    else:\n",
    "        results['eval_accuracy'] = 0\n",
    "        results['eval_f1_macro'] = 0\n",
    "        results['eval_f1_weighted'] = 0\n",
    "\n",
    "    return results\n",
    "\n",
    "def show_classification_report(svm_model, X_test, y_test, X_eval, y_eval, authors):\n",
    "    \"\"\"Mostra classification report dettagliato\"\"\"\n",
    "\n",
    "    print(\"\\nüìä CLASSIFICATION REPORTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if X_test.shape[0] > 0:\n",
    "        print(\"\\nüß™ TEST SET:\")\n",
    "        test_pred = svm_model.predict(X_test)\n",
    "        print(classification_report(y_test, test_pred, target_names=authors, zero_division=0))\n",
    "\n",
    "    if X_eval.shape[0] > 0:\n",
    "        print(\"\\nüéØ EVAL SET:\")\n",
    "        eval_pred = svm_model.predict(X_eval)\n",
    "        print(classification_report(y_eval, eval_pred, target_names=authors, zero_division=0))\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZZAZIONE RISULTATI\n",
    "# ============================================================================\n",
    "\n",
    "def plot_configuration_comparison(all_results):\n",
    "    \"\"\"Plotta confronto tra configurazioni\"\"\"\n",
    "\n",
    "    config_names = [r['config_name'] for r in all_results]\n",
    "    cv_accuracy = [r['cv_accuracy_mean'] for r in all_results]\n",
    "    cv_f1_macro = [r['cv_f1_macro_mean'] for r in all_results]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Accuracy\n",
    "    bars1 = ax1.bar(range(len(config_names)), cv_accuracy, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Cross-Validation Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xticks(range(len(config_names)))\n",
    "    ax1.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    # Aggiungi valori sopra le barre\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # F1-macro\n",
    "    bars2 = ax2.bar(range(len(config_names)), cv_f1_macro, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Cross-Validation F1-Macro')\n",
    "    ax2.set_ylabel('F1-Macro')\n",
    "    ax2.set_xticks(range(len(config_names)))\n",
    "    ax2.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # Aggiungi valori sopra le barre\n",
    "    for i, bar in enumerate(bars2):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_results_table(all_results):\n",
    "    \"\"\"Stampa tabella riassuntiva risultati\"\"\"\n",
    "\n",
    "    print(\"\\nüìä TABELLA RIASSUNTIVA RISULTATI\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    header = f\"{'Configurazione':<20} {'CV Accuracy':<12} {'CV F1-Macro':<12} {'Baseline Acc':<12} {'Improvement':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for result in all_results:\n",
    "        row = (f\"{result['config_name']:<20} \"\n",
    "               f\"{result['cv_accuracy_mean']:.4f}¬±{result['cv_accuracy_std']:.3f} \"\n",
    "               f\"{result['cv_f1_macro_mean']:.4f}¬±{result['cv_f1_macro_std']:.3f} \"\n",
    "               f\"{result['baseline_accuracy']:.4f}      \"\n",
    "               f\"+{result['improvement_accuracy']:.4f}\")\n",
    "        print(row)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "def find_best_configuration(all_results, metric='cv_f1_macro_mean'):\n",
    "    \"\"\"Trova la configurazione migliore\"\"\"\n",
    "\n",
    "    best_result = max(all_results, key=lambda x: x[metric])\n",
    "\n",
    "    print(f\"\\nüèÜ MIGLIORE CONFIGURAZIONE (per {metric}):\")\n",
    "    print(f\"  üìõ Nome: {best_result['config_name']}\")\n",
    "    print(f\"  üéØ CV Accuracy: {best_result['cv_accuracy_mean']:.4f} ¬± {best_result['cv_accuracy_std']:.4f}\")\n",
    "    print(f\"  üìä CV F1-Macro: {best_result['cv_f1_macro_mean']:.4f} ¬± {best_result['cv_f1_macro_std']:.4f}\")\n",
    "    print(f\"  üöÄ Miglioramento vs baseline: +{best_result['improvement_accuracy']:.4f} (Acc), +{best_result['improvement_f1_macro']:.4f} (F1)\")\n",
    "\n",
    "    return best_result"
   ],
   "metadata": {
    "id": "ak4Me9owW5im"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# APPLICAZIONE COMPLETA SVM N-GRAMMI AUTHORSHIP ATTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_ngram_authorship_attribution(base_path):\n",
    "    \"\"\"\n",
    "    Pipeline completa per authorship attribution con n-grammi\n",
    "\n",
    "    Args:\n",
    "        base_path: percorso alla cartella con training_set/, test_set/, eval_set/\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üöÄ INIZIO PIPELINE SVM N-GRAMMI AUTHORSHIP ATTRIBUTION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. CARICAMENTO DATI\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\nüìÇ FASE 1: CARICAMENTO DATI\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    datasets = load_dataset_from_structure(base_path)\n",
    "\n",
    "    train_docs = datasets['training_set']\n",
    "    test_docs = datasets['test_set']\n",
    "    eval_docs = datasets['eval_set']\n",
    "\n",
    "    # Verifica autori\n",
    "    all_authors = set()\n",
    "    for docs in [train_docs, test_docs, eval_docs]:\n",
    "        all_authors.update([doc.author for doc in docs])\n",
    "\n",
    "    authors = sorted(list(all_authors))\n",
    "\n",
    "    print(f\"\\nüìä RIEPILOGO DATASET:\")\n",
    "    print(f\"  üë• Autori: {authors}\")\n",
    "    print(f\"  üèãÔ∏è  Training: {len(train_docs)} documenti\")\n",
    "    print(f\"  üß™ Test: {len(test_docs)} documenti\")\n",
    "    print(f\"  üéØ Eval: {len(eval_docs)} documenti\")\n",
    "\n",
    "    if len(train_docs) == 0:\n",
    "        print(\"‚ùå ERRORE: Nessun documento di training trovato!\")\n",
    "        return\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. CONFIGURAZIONI SPERIMENTALI\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è  FASE 2: CONFIGURAZIONI SPERIMENTALI\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    configs = get_experimental_configurations()\n",
    "\n",
    "    print(f\"üîß Configurazioni da testare: {len(configs)}\")\n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f\"  {i}. {config['name']}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. CROSS-VALIDATION DELLE CONFIGURAZIONI\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\nüîÑ FASE 3: CROSS-VALIDATION SU TRAINING SET\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f\"\\nüß™ Test configurazione {i}/{len(configs)}: {config['name']}\")\n",
    "\n",
    "        try:\n",
    "            # Prepara dati\n",
    "            X_train, y_train, vectorizer, scaler = prepare_data_for_training(train_docs, config)\n",
    "\n",
    "            # Cross-validation\n",
    "            cv_results = cross_validate_config(X_train, y_train, config['name'])\n",
    "            all_results.append(cv_results)\n",
    "\n",
    "            # Stampa risultati brevi\n",
    "            print(f\"  ‚úÖ CV Accuracy: {cv_results['cv_accuracy_mean']:.4f} ¬± {cv_results['cv_accuracy_std']:.4f}\")\n",
    "            print(f\"  üìä CV F1-Macro: {cv_results['cv_f1_macro_mean']:.4f} ¬± {cv_results['cv_f1_macro_std']:.4f}\")\n",
    "            print(f\"  üöÄ Miglioramento: +{cv_results['improvement_accuracy']:.4f} (Acc), +{cv_results['improvement_f1_macro']:.4f} (F1)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Errore: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # ========================================================================\n",
    "    # 4. CONFRONTO RISULTATI E SELEZIONE MIGLIOR CONFIGURAZIONE\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\nüìä FASE 4: CONFRONTO RISULTATI\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"‚ùå ERRORE: Nessuna configurazione completata con successo!\")\n",
    "        return\n",
    "\n",
    "    # Stampa tabella riassuntiva\n",
    "    print_results_table(all_results)\n",
    "\n",
    "    # Trova miglior configurazione\n",
    "    best_config_result = find_best_configuration(all_results, 'cv_f1_macro_mean')\n",
    "    best_config_name = best_config_result['config_name']\n",
    "\n",
    "    # Trova configurazione corrispondente\n",
    "    best_config = None\n",
    "    for config in configs:\n",
    "        if config['name'] == best_config_name:\n",
    "            best_config = config\n",
    "            break\n",
    "\n",
    "    if best_config is None:\n",
    "        print(\"‚ùå ERRORE: Configurazione migliore non trovata!\")\n",
    "        return\n",
    "\n",
    "    # Visualizzazione grafica\n",
    "    if len(all_results) > 1:\n",
    "        plot_configuration_comparison(all_results)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 5. TRAINING FINALE E VALUTAZIONE SU TEST/EVAL SET\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\nüèÜ FASE 5: VALUTAZIONE CONFIGURAZIONE MIGLIORE\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(f\"üîß Training finale con configurazione: {best_config['name']}\")\n",
    "\n",
    "    try:\n",
    "        # Prepara dati training con miglior configurazione\n",
    "        X_train, y_train, vectorizer, scaler = prepare_data_for_training(train_docs, best_config)\n",
    "\n",
    "        # Training del modello finale\n",
    "        print(\"üèãÔ∏è  Training modello finale...\")\n",
    "        final_svm_model = train_svm_model(X_train, y_train)\n",
    "        print(\"‚úÖ Training completato!\")\n",
    "\n",
    "        # Prepara dati test e eval\n",
    "        if test_docs:\n",
    "            X_test, y_test = prepare_test_data(test_docs, vectorizer, scaler, best_config)\n",
    "            print(f\"  üìä Test set preparato: {X_test.shape}\")\n",
    "        else:\n",
    "            X_test, y_test = np.array([]).reshape(0, X_train.shape[1]), np.array([])\n",
    "            print(\"  ‚ö†Ô∏è  Test set vuoto\")\n",
    "\n",
    "        if eval_docs:\n",
    "            X_eval, y_eval = prepare_test_data(eval_docs, vectorizer, scaler, best_config)\n",
    "            print(f\"  üìä Eval set preparato: {X_eval.shape}\")\n",
    "        else:\n",
    "            X_eval, y_eval = np.array([]).reshape(0, X_train.shape[1]), np.array([])\n",
    "            print(\"  ‚ö†Ô∏è  Eval set vuoto\")\n",
    "\n",
    "        # Valutazione finale\n",
    "        final_results = evaluate_on_test_set(\n",
    "            final_svm_model, X_test, y_test, X_eval, y_eval, best_config['name']\n",
    "        )\n",
    "\n",
    "        # ====================================================================\n",
    "        # 6. RISULTATI FINALI\n",
    "        # ====================================================================\n",
    "\n",
    "        print(f\"\\nüìà FASE 6: RISULTATI FINALI\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        print(f\"\\nüèÜ MIGLIORE CONFIGURAZIONE: {best_config['name']}\")\n",
    "        print(f\"üìä Cross-Validation:\")\n",
    "        print(f\"  üéØ Accuracy: {best_config_result['cv_accuracy_mean']:.4f} ¬± {best_config_result['cv_accuracy_std']:.4f}\")\n",
    "        print(f\"  üìä F1-Macro: {best_config_result['cv_f1_macro_mean']:.4f} ¬± {best_config_result['cv_f1_macro_std']:.4f}\")\n",
    "        print(f\"  üìä F1-Weighted: {best_config_result['cv_f1_weighted_mean']:.4f} ¬± {best_config_result['cv_f1_weighted_std']:.4f}\")\n",
    "\n",
    "        if X_test.shape[0] > 0:\n",
    "            print(f\"\\nüß™ Test Set:\")\n",
    "            print(f\"  üéØ Accuracy: {final_results['test_accuracy']:.4f}\")\n",
    "            print(f\"  üìä F1-Macro: {final_results['test_f1_macro']:.4f}\")\n",
    "            print(f\"  üìä F1-Weighted: {final_results['test_f1_weighted']:.4f}\")\n",
    "\n",
    "        if X_eval.shape[0] > 0:\n",
    "            print(f\"\\nüéØ Eval Set:\")\n",
    "            print(f\"  üéØ Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "            print(f\"  üìä F1-Macro: {final_results['eval_f1_macro']:.4f}\")\n",
    "            print(f\"  üìä F1-Weighted: {final_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "        # Classification report dettagliato\n",
    "        show_classification_report(final_svm_model, X_test, y_test, X_eval, y_eval, authors)\n",
    "\n",
    "        # ====================================================================\n",
    "        # 7. ANALISI FEATURE IMPORTANTI\n",
    "        # ====================================================================\n",
    "\n",
    "        print(f\"\\nüîç FASE 7: ANALISI FEATURE IMPORTANCE\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        analyze_feature_importance(final_svm_model, vectorizer, authors, top_n=15)\n",
    "\n",
    "        print(f\"\\n‚úÖ PIPELINE COMPLETATA!\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        return {\n",
    "            'best_config': best_config,\n",
    "            'final_model': final_svm_model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'scaler': scaler,\n",
    "            'results': {\n",
    "                'cv_results': best_config_result,\n",
    "                'final_results': final_results\n",
    "            },\n",
    "            'all_results': all_results\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRORE nella valutazione finale: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def analyze_feature_importance(svm_model, vectorizer, authors, top_n=15):\n",
    "    \"\"\"Analizza le feature pi√π importanti per ogni autore\"\"\"\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefs = svm_model.coef_\n",
    "\n",
    "    print(f\"üîç TOP {top_n} FEATURE PI√ô DISCRIMINANTI:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for idx, author in enumerate(authors):\n",
    "        print(f\"\\nüìä {author.upper()}:\")\n",
    "\n",
    "        # Coefficienti per questo autore\n",
    "        author_coefs = coefs[idx] if len(coefs.shape) > 1 else coefs\n",
    "\n",
    "        # Crea coppie (feature, coefficiente) e ordina per valore assoluto\n",
    "        feature_importance = list(zip(feature_names, author_coefs))\n",
    "        feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "        # Mostra top N\n",
    "        for i, (feature, coef) in enumerate(feature_importance[:top_n], 1):\n",
    "            direction = \"+\" if coef > 0 else \"-\"\n",
    "            print(f\"  {i:2d}. {feature:<35} {direction} {abs(coef):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ESEMPIO DI UTILIZZO\n",
    "# ============================================================================\n",
    "\n",
    "def main_example():\n",
    "    \"\"\"Esempio di utilizzo della pipeline completa\"\"\"\n",
    "\n",
    "    # MODIFICA QUESTO PERCORSO CON IL TUO!\n",
    "    base_path = \"/content/drive/MyDrive/16590/dataset_authorship_finale\"\n",
    "\n",
    "    # Per Google Colab, prima monta il drive:\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "\n",
    "    # Esegui pipeline completa\n",
    "    results = run_ngram_authorship_attribution(base_path)\n",
    "\n",
    "    if results:\n",
    "        print(\"\\nüéâ Pipeline completata con successo!\")\n",
    "        print(f\"üìä Miglior configurazione: {results['best_config']['name']}\")\n",
    "\n",
    "        # Puoi accedere ai risultati:\n",
    "        # - results['final_model']: modello SVM finale\n",
    "        # - results['vectorizer']: vectorizer per nuove predizioni\n",
    "        # - results['scaler']: scaler per normalizzazione\n",
    "        # - results['all_results']: tutti i risultati delle configurazioni\n",
    "    else:\n",
    "        print(\"‚ùå Pipeline fallita!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PER ESEGUIRE SU COLAB\n",
    "# ============================================================================\n",
    "\n",
    "# Decomenta e modifica il percorso:\n",
    "#\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# # Imposta il tuo percorso\n",
    "base_path = \"/content/drive/MyDrive/libri_conlu/dataset_authorship_finale_profilato\"\n",
    "#\n",
    "# # Esegui la pipeline\n",
    "results = run_ngram_authorship_attribution(base_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yWV5WK1TXPbH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756415866294,
     "user_tz": -120,
     "elapsed": 1141129,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "1ad4aa62-7e6d-4374-c3b9-13f282af64c4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ INIZIO PIPELINE SVM N-GRAMMI AUTHORSHIP ATTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "üìÇ FASE 1: CARICAMENTO DATI\n",
      "----------------------------------------\n",
      "üìÇ Caricamento training_set...\n",
      "  üìÑ primo autore: 1802 documenti caricati\n",
      "  üìÑ secondo autore: 2151 documenti caricati\n",
      "  üìÑ terzo autore: 1552 documenti caricati\n",
      "  ‚úÖ training_set: 5505 documenti totali\n",
      "üìÇ Caricamento test_set...\n",
      "  üìÑ primo autore: 210 documenti caricati\n",
      "  üìÑ secondo autore: 141 documenti caricati\n",
      "  üìÑ terzo autore: 128 documenti caricati\n",
      "  ‚úÖ test_set: 479 documenti totali\n",
      "üìÇ Caricamento eval_set...\n",
      "  üìÑ primo autore: 211 documenti caricati\n",
      "  üìÑ secondo autore: 142 documenti caricati\n",
      "  üìÑ terzo autore: 128 documenti caricati\n",
      "  ‚úÖ eval_set: 481 documenti totali\n",
      "\n",
      "üìä RIEPILOGO DATASET:\n",
      "  üë• Autori: ['primo autore', 'secondo autore', 'terzo autore']\n",
      "  üèãÔ∏è  Training: 5505 documenti\n",
      "  üß™ Test: 479 documenti\n",
      "  üéØ Eval: 481 documenti\n",
      "\n",
      "‚öôÔ∏è  FASE 2: CONFIGURAZIONI SPERIMENTALI\n",
      "----------------------------------------\n",
      "üîß Configurazioni da testare: 8\n",
      "  1. Char_1-3\n",
      "  2. Word_1-2\n",
      "  3. Lemma_1-2\n",
      "  4. POS_1-3\n",
      "  5. UPOS_1-3\n",
      "  6. Word_Char_Combined\n",
      "  7. Lemma_POS_Combined\n",
      "  8. All_Features\n",
      "\n",
      "üîÑ FASE 3: CROSS-VALIDATION SU TRAINING SET\n",
      "----------------------------------------\n",
      "\n",
      "üß™ Test configurazione 1/8: Char_1-3\n",
      "üîß Estrazione features con configurazione: Char_1-3\n",
      "  üîΩ Feature filtrate: 9428 ‚Üí 7816 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 7816)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.8699 ¬± 0.0057\n",
      "  üìä CV F1-Macro: 0.8669 ¬± 0.0067\n",
      "  üöÄ Miglioramento: +0.4792 (Acc), +0.6797 (F1)\n",
      "\n",
      "üß™ Test configurazione 2/8: Word_1-2\n",
      "üîß Estrazione features con configurazione: Word_1-2\n",
      "  üîΩ Feature filtrate: 181003 ‚Üí 60563 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 60563)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.9117 ¬± 0.0082\n",
      "  üìä CV F1-Macro: 0.9087 ¬± 0.0084\n",
      "  üöÄ Miglioramento: +0.5210 (Acc), +0.7215 (F1)\n",
      "\n",
      "üß™ Test configurazione 3/8: Lemma_1-2\n",
      "üîß Estrazione features con configurazione: Lemma_1-2\n",
      "  üîΩ Feature filtrate: 155011 ‚Üí 54160 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 54160)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.9112 ¬± 0.0070\n",
      "  üìä CV F1-Macro: 0.9087 ¬± 0.0071\n",
      "  üöÄ Miglioramento: +0.5204 (Acc), +0.7215 (F1)\n",
      "\n",
      "üß™ Test configurazione 4/8: POS_1-3\n",
      "üîß Estrazione features con configurazione: POS_1-3\n",
      "  üîΩ Feature filtrate: 14147 ‚Üí 9927 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 9927)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.6763 ¬± 0.0056\n",
      "  üìä CV F1-Macro: 0.6690 ¬± 0.0067\n",
      "  üöÄ Miglioramento: +0.2856 (Acc), +0.4818 (F1)\n",
      "\n",
      "üß™ Test configurazione 5/8: UPOS_1-3\n",
      "üîß Estrazione features con configurazione: UPOS_1-3\n",
      "  üîΩ Feature filtrate: 3063 ‚Üí 2649 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 2649)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.6300 ¬± 0.0091\n",
      "  üìä CV F1-Macro: 0.6228 ¬± 0.0088\n",
      "  üöÄ Miglioramento: +0.2392 (Acc), +0.4356 (F1)\n",
      "\n",
      "üß™ Test configurazione 6/8: Word_Char_Combined\n",
      "üîß Estrazione features con configurazione: Word_Char_Combined\n",
      "  üîΩ Feature filtrate: 190349 ‚Üí 68304 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 68304)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.9135 ¬± 0.0067\n",
      "  üìä CV F1-Macro: 0.9113 ¬± 0.0068\n",
      "  üöÄ Miglioramento: +0.5228 (Acc), +0.7241 (F1)\n",
      "\n",
      "üß™ Test configurazione 7/8: Lemma_POS_Combined\n",
      "üîß Estrazione features con configurazione: Lemma_POS_Combined\n",
      "  üîΩ Feature filtrate: 169158 ‚Üí 64087 (min_docs=2)\n",
      "  üìä Matrice features: (5505, 64087)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.8981 ¬± 0.0071\n",
      "  üìä CV F1-Macro: 0.8949 ¬± 0.0078\n",
      "  üöÄ Miglioramento: +0.5074 (Acc), +0.7077 (F1)\n",
      "\n",
      "üß™ Test configurazione 8/8: All_Features\n",
      "üîß Estrazione features con configurazione: All_Features\n",
      "  üîΩ Feature filtrate: 359875 ‚Üí 81479 (min_docs=3)\n",
      "  üìä Matrice features: (5505, 81479)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "  ‚úÖ CV Accuracy: 0.9172 ¬± 0.0056\n",
      "  üìä CV F1-Macro: 0.9145 ¬± 0.0060\n",
      "  üöÄ Miglioramento: +0.5264 (Acc), +0.7272 (F1)\n",
      "\n",
      "üìä FASE 4: CONFRONTO RISULTATI\n",
      "----------------------------------------\n",
      "\n",
      "üìä TABELLA RIASSUNTIVA RISULTATI\n",
      "====================================================================================================\n",
      "Configurazione       CV Accuracy  CV F1-Macro  Baseline Acc Improvement \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Char_1-3             0.8699¬±0.006 0.8669¬±0.007 0.3907      +0.4792\n",
      "Word_1-2             0.9117¬±0.008 0.9087¬±0.008 0.3907      +0.5210\n",
      "Lemma_1-2            0.9112¬±0.007 0.9087¬±0.007 0.3907      +0.5204\n",
      "POS_1-3              0.6763¬±0.006 0.6690¬±0.007 0.3907      +0.2856\n",
      "UPOS_1-3             0.6300¬±0.009 0.6228¬±0.009 0.3907      +0.2392\n",
      "Word_Char_Combined   0.9135¬±0.007 0.9113¬±0.007 0.3907      +0.5228\n",
      "Lemma_POS_Combined   0.8981¬±0.007 0.8949¬±0.008 0.3907      +0.5074\n",
      "All_Features         0.9172¬±0.006 0.9145¬±0.006 0.3907      +0.5264\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üèÜ MIGLIORE CONFIGURAZIONE (per cv_f1_macro_mean):\n",
      "  üìõ Nome: All_Features\n",
      "  üéØ CV Accuracy: 0.9172 ¬± 0.0056\n",
      "  üìä CV F1-Macro: 0.9145 ¬± 0.0060\n",
      "  üöÄ Miglioramento vs baseline: +0.5264 (Acc), +0.7272 (F1)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "üèÜ FASE 5: VALUTAZIONE CONFIGURAZIONE MIGLIORE\n",
      "----------------------------------------\n",
      "üîß Training finale con configurazione: All_Features\n",
      "üîß Estrazione features con configurazione: All_Features\n",
      "  üîΩ Feature filtrate: 359875 ‚Üí 81479 (min_docs=3)\n",
      "  üìä Matrice features: (5505, 81479)\n",
      "  üë• Distribuzione classi: {np.str_('primo autore'): np.int64(1802), np.str_('secondo autore'): np.int64(2151), np.str_('terzo autore'): np.int64(1552)}\n",
      "üèãÔ∏è  Training modello finale...\n",
      "‚úÖ Training completato!\n",
      "  üìä Test set preparato: (479, 81479)\n",
      "  üìä Eval set preparato: (481, 81479)\n",
      "\n",
      "üìà FASE 6: RISULTATI FINALI\n",
      "----------------------------------------\n",
      "\n",
      "üèÜ MIGLIORE CONFIGURAZIONE: All_Features\n",
      "üìä Cross-Validation:\n",
      "  üéØ Accuracy: 0.9172 ¬± 0.0056\n",
      "  üìä F1-Macro: 0.9145 ¬± 0.0060\n",
      "  üìä F1-Weighted: 0.9168 ¬± 0.0056\n",
      "\n",
      "üß™ Test Set:\n",
      "  üéØ Accuracy: 0.9019\n",
      "  üìä F1-Macro: 0.8977\n",
      "  üìä F1-Weighted: 0.9024\n",
      "\n",
      "üéØ Eval Set:\n",
      "  üéØ Accuracy: 0.9023\n",
      "  üìä F1-Macro: 0.8953\n",
      "  üìä F1-Weighted: 0.9017\n",
      "\n",
      "üìä CLASSIFICATION REPORTS\n",
      "==================================================\n",
      "\n",
      "üß™ TEST SET:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  primo autore       0.97      0.89      0.93       210\n",
      "secondo autore       0.83      0.96      0.89       141\n",
      "  terzo autore       0.89      0.86      0.87       128\n",
      "\n",
      "      accuracy                           0.90       479\n",
      "     macro avg       0.90      0.90      0.90       479\n",
      "  weighted avg       0.91      0.90      0.90       479\n",
      "\n",
      "\n",
      "üéØ EVAL SET:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  primo autore       0.93      0.93      0.93       211\n",
      "secondo autore       0.86      0.94      0.90       142\n",
      "  terzo autore       0.90      0.81      0.85       128\n",
      "\n",
      "      accuracy                           0.90       481\n",
      "     macro avg       0.90      0.90      0.90       481\n",
      "  weighted avg       0.90      0.90      0.90       481\n",
      "\n",
      "\n",
      "üîç FASE 7: ANALISI FEATURE IMPORTANCE\n",
      "----------------------------------------\n",
      "üîç TOP 15 FEATURE PI√ô DISCRIMINANTI:\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä PRIMO AUTORE:\n",
      "   1. CHAR_3_rnd                          + 0.1756\n",
      "   2. CHAR_2_yk                           + 0.1752\n",
      "   3. CHAR_3_dyk                          + 0.1752\n",
      "   4. CHAR_3_yke                          + 0.1752\n",
      "   5. LEMMA_1_Thorndyke                   + 0.1713\n",
      "   6. WORD_1_Thorndyke                    + 0.1713\n",
      "   7. CHAR_3_ndy                          + 0.1427\n",
      "   8. CHAR_2_dy                           + 0.1313\n",
      "   9. CHAR_1_                             - 0.1290\n",
      "  10. CHAR_2_ct                           + 0.1288\n",
      "  11. POS_3_''_VBD_PRP                    + 0.1274\n",
      "  12. LEMMA_1_we                          + 0.1258\n",
      "  13. POS_2_CC_PRP                        + 0.1177\n",
      "  14. POS_3_IN_PRP_VBD                    + 0.1162\n",
      "  15. CHAR_3_dis                          + 0.1157\n",
      "\n",
      "üìä SECONDO AUTORE:\n",
      "   1. UPOS_2_PUNCT_CCONJ                  - 0.1917\n",
      "   2. POS_2_,_CC                          - 0.1842\n",
      "   3. LEMMA_1_she                         + 0.1609\n",
      "   4. CHAR_2_ ‚Äî                           - 0.1451\n",
      "   5. CHAR_2_‚Äî                            - 0.1426\n",
      "   6. WORD_1_replied                      - 0.1405\n",
      "   7. CHAR_1_‚Äî                            - 0.1403\n",
      "   8. CHAR_3_ ‚Äî                           - 0.1399\n",
      "   9. WORD_1_‚Äî                            - 0.1399\n",
      "  10. WORD_1_Barnes                       + 0.1330\n",
      "  11. LEMMA_1_Barnes                      + 0.1322\n",
      "  12. CHAR_1_6                            - 0.1316\n",
      "  13. WORD_1_‚Äôs                           - 0.1286\n",
      "  14. CHAR_2_11                           - 0.1285\n",
      "  15. CHAR_3_ela                          + 0.1284\n",
      "\n",
      "üìä TERZO AUTORE:\n",
      "   1. CHAR_1_‚Äî                            + 0.1882\n",
      "   2. CHAR_2_ ‚Äî                           + 0.1784\n",
      "   3. CHAR_2_‚Äî                            + 0.1771\n",
      "   4. CHAR_2_mm                           + 0.1754\n",
      "   5. CHAR_3_ ‚Äî                           + 0.1583\n",
      "   6. WORD_1_‚Äî                            + 0.1583\n",
      "   7. WORD_2_he_said                      + 0.1490\n",
      "   8. POS_2_,_CC                          + 0.1489\n",
      "   9. UPOS_2_PUNCT_CCONJ                  + 0.1462\n",
      "  10. CHAR_1_6                            + 0.1410\n",
      "  11. LEMMA_1_Hemming                     + 0.1348\n",
      "  12. CHAR_3_ina                          - 0.1324\n",
      "  13. CHAR_3_pe                           - 0.1319\n",
      "  14. CHAR_2_61                           + 0.1307\n",
      "  15. CHAR_2_11                           + 0.1304\n",
      "\n",
      "‚úÖ PIPELINE COMPLETATA!\n",
      "======================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD FASTTEXT MODEL TO GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "!pip install fasttext\n",
    "import os\n",
    "import shutil\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# Configura percorsi\n",
    "DRIVE_BASE = \"/content/drive/MyDrive\"\n",
    "MODELS_DIR = os.path.join(DRIVE_BASE, \"fasttext_models\")\n",
    "MODEL_NAME = \"cc.en.300.bin\"\n",
    "MODEL_PATH = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "\n",
    "# Crea directory se non esiste\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Directory modelli: {MODELS_DIR}\")\n",
    "print(f\"üéØ Percorso modello: {MODEL_PATH}\")\n",
    "\n",
    "# Controlla se il modello esiste gi√†\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"‚úÖ Il modello esiste gi√† nel Drive!\")\n",
    "    print(f\"üìä Dimensione file: {os.path.getsize(MODEL_PATH) / (1024**3):.2f} GB\")\n",
    "\n",
    "    # Carica il modello esistente\n",
    "    print(\"‚è≥ Caricamento modello dal Drive...\")\n",
    "    ft = fasttext.load_model(MODEL_PATH)\n",
    "    print(f\"‚úÖ Modello caricato! Dimensione vettore: {ft.get_dimension()}\")\n",
    "\n",
    "else:\n",
    "    print(\"üîÑ Il modello non esiste, procedo con il download...\")\n",
    "\n",
    "    # Download temporaneo nella directory di default\n",
    "    print(\"‚è≥ Download in corso... (questo pu√≤ richiedere diversi minuti)\")\n",
    "    fasttext.util.download_model('en', if_exists='ignore')\n",
    "\n",
    "    # Trova il file scaricato\n",
    "    temp_model_path = MODEL_NAME  # FastText scarica nella directory corrente\n",
    "\n",
    "    if os.path.exists(temp_model_path):\n",
    "        print(f\"‚úÖ Download completato!\")\n",
    "        print(f\"üìä Dimensione file: {os.path.getsize(temp_model_path) / (1024**3):.2f} GB\")\n",
    "\n",
    "        # Sposta nel Drive\n",
    "        print(f\"üöö Spostamento nel Drive: {MODEL_PATH}\")\n",
    "        shutil.move(temp_model_path, MODEL_PATH)\n",
    "        print(\"‚úÖ Modello salvato nel Drive!\")\n",
    "\n",
    "        # Carica il modello\n",
    "        print(\"‚è≥ Caricamento modello...\")\n",
    "        ft = fasttext.load_model(MODEL_PATH)\n",
    "        print(f\"‚úÖ Modello caricato! Dimensione vettore: {ft.get_dimension()}\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Errore: file del modello non trovato dopo il download\")\n",
    "        ft = None\n",
    "\n",
    "# Test del modello\n",
    "if 'ft' in locals() and ft is not None:\n",
    "    print(\"\\nüß™ TEST DEL MODELLO:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    test_words = [\"author\", \"writing\", \"style\", \"book\", \"novel\"]\n",
    "\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            vec = ft.get_word_vector(word)\n",
    "            print(f\"‚úÖ '{word}': shape {vec.shape}, sample: {vec[:3].round(3)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore con '{word}': {e}\")\n",
    "\n",
    "    print(f\"\\nüìà Vocabolario: ~{len(ft.get_words())} parole\")\n",
    "    print(f\"üî§ Dimensione embedding: {ft.get_dimension()}\")\n",
    "\n",
    "    # Funzione helper per uso futuro\n",
    "    def get_embeddings_dict(model, words_list):\n",
    "        \"\"\"Crea dizionario embeddings per una lista di parole\"\"\"\n",
    "        embeddings = {}\n",
    "        for word in words_list:\n",
    "            embeddings[word] = model.get_word_vector(word)\n",
    "        return embeddings\n",
    "\n",
    "    print(\"\\nüí° Per usare il modello in futuro:\")\n",
    "    print(f\"ft = fasttext.load_model('{MODEL_PATH}')\")\n",
    "    print(\"embeddings = get_embeddings_dict(ft, your_vocabulary)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Modello non disponibile\")\n",
    "\n",
    "print(f\"\\nüìç Modello salvato in: {MODEL_PATH}\")\n",
    "print(\"üîÑ Il modello rimarr√† nel tuo Drive per usi futuri!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q72hEAIK99GA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756737896634,
     "user_tz": -120,
     "elapsed": 140728,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "a7f4a6cc-15b4-406c-f32d-12f07e5c9da5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: fasttext in /usr/local/lib/python3.12/dist-packages (0.9.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.12/dist-packages (from fasttext) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext) (75.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fasttext) (2.0.2)\n",
      "üìÅ Directory modelli: /content/drive/MyDrive/fasttext_models\n",
      "üéØ Percorso modello: /content/drive/MyDrive/fasttext_models/cc.en.300.bin\n",
      "‚úÖ Il modello esiste gi√† nel Drive!\n",
      "üìä Dimensione file: 6.74 GB\n",
      "‚è≥ Caricamento modello dal Drive...\n",
      "‚úÖ Modello caricato! Dimensione vettore: 300\n",
      "\n",
      "üß™ TEST DEL MODELLO:\n",
      "----------------------------------------\n",
      "‚úÖ 'author': shape (300,), sample: [-0.026  0.06   0.004]\n",
      "‚úÖ 'writing': shape (300,), sample: [-0.007  0.025  0.056]\n",
      "‚úÖ 'style': shape (300,), sample: [0.097 0.014 0.039]\n",
      "‚úÖ 'book': shape (300,), sample: [-0.123  0.105  0.035]\n",
      "‚úÖ 'novel': shape (300,), sample: [-0.04   0.02   0.101]\n",
      "\n",
      "üìà Vocabolario: ~2000000 parole\n",
      "üî§ Dimensione embedding: 300\n",
      "\n",
      "üí° Per usare il modello in futuro:\n",
      "ft = fasttext.load_model('/content/drive/MyDrive/fasttext_models/cc.en.300.bin')\n",
      "embeddings = get_embeddings_dict(ft, your_vocabulary)\n",
      "\n",
      "üìç Modello salvato in: /content/drive/MyDrive/fasttext_models/cc.en.300.bin\n",
      "üîÑ Il modello rimarr√† nel tuo Drive per usi futuri!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CLASSIFICATORE SVM CON WORD EMBEDDINGS - AUTORE ADATTATO PER TOKEN SENZA POS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from collections import Counter, namedtuple\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# ======= STRUTTURE DATI =======\n",
    "Token = namedtuple('Token', ['word', 'pos'])  # pos ignorato\n",
    "Document = namedtuple('Document', ['tokens', 'author', 'split'])\n",
    "\n",
    "# ======= CARICAMENTO DATI =======\n",
    "def load_documents(dataset_path):\n",
    "    documents = []\n",
    "    set_mapping = {'training_set': 'training', 'test_set': 'test', 'eval_set': 'eval'}\n",
    "\n",
    "    for set_name, split in set_mapping.items():\n",
    "        set_path = os.path.join(dataset_path, set_name)\n",
    "        if not os.path.exists(set_path):\n",
    "            continue\n",
    "        for author in os.listdir(set_path):\n",
    "            author_path = os.path.join(set_path, author)\n",
    "            if not os.path.isdir(author_path):\n",
    "                continue\n",
    "            for fname in os.listdir(author_path):\n",
    "                if fname.endswith('.txt'):\n",
    "                    with open(os.path.join(author_path, fname), 'r', encoding='utf-8') as f:\n",
    "                        text = f.read().strip()\n",
    "                    words = [w.strip('.,!?\":;()[]{}').lower() for w in text.split() if w.strip('.,!?\":;()[]{}')]\n",
    "                    if len(words) > 10:\n",
    "                        tokens = [Token(word=w, pos='WORD') for w in words]\n",
    "                        documents.append(Document(tokens=tokens, author=author, split=split))s\n",
    "    return documents\n",
    "\n",
    "# ======= VOCABOLARIO E EMBEDDINGS =======\n",
    "def extract_vocab(documents, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for doc in documents:\n",
    "        for token in doc.tokens:\n",
    "            counter[token.word] += 1\n",
    "    return [w for w, c in counter.items() if c >= min_freq]\n",
    "\n",
    "def create_embeddings_dict(ft_model, vocab):\n",
    "    return {w: ft_model.get_word_vector(w) for w in vocab}\n",
    "\n",
    "# ======= CONFIGURAZIONI EMBEDDINGS =======\n",
    "def get_configs():\n",
    "    return [\n",
    "        {'name': 'Mean_All_Words', 'method': 'mean'},\n",
    "        {'name': 'Max_Pooling', 'method': 'max'},\n",
    "        {'name': 'Sum_All_Words', 'method': 'sum'}\n",
    "    ]\n",
    "\n",
    "# ======= ESTRAZIONE FEATURES =======\n",
    "def extract_features(doc, embeddings, config):\n",
    "    vectors = [embeddings[t.word] for t in doc.tokens if t.word in embeddings]\n",
    "    if not vectors:\n",
    "        return np.zeros(300)  # FastText dimension\n",
    "    arr = np.array(vectors)\n",
    "    if config['method'] == 'mean':\n",
    "        return np.mean(arr, axis=0)\n",
    "    elif config['method'] == 'max':\n",
    "        return np.max(arr, axis=0)\n",
    "    elif config['method'] == 'sum':\n",
    "        return np.sum(arr, axis=0)\n",
    "\n",
    "# ======= PREPARAZIONE DATI =======\n",
    "def prepare_data(docs, embeddings, config):\n",
    "    X, y = [], []\n",
    "    for doc in docs:\n",
    "        X.append(extract_features(doc, embeddings, config))\n",
    "        y.append(doc.author)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ======= CROSS-VALIDATION =======\n",
    "def cross_validate_model(X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    svm = LinearSVC(dual=False, max_iter=5000, class_weight='balanced', random_state=42)\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring = ['accuracy', 'f1_macro']\n",
    "    cv_results = cross_validate(svm, X_scaled, y, cv=cv, scoring=scoring)\n",
    "    baseline = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "    baseline_results = cross_validate(baseline, X_scaled, y, cv=cv, scoring=['accuracy'])\n",
    "    return cv_results, baseline_results, scaler\n",
    "\n",
    "# ======= ESPERIMENTO PRINCIPALE =======\n",
    "def run_experiment(documents, embeddings):\n",
    "    train_docs = [d for d in documents if d.split=='training']\n",
    "    test_docs = [d for d in documents if d.split=='test']\n",
    "    eval_docs = [d for d in documents if d.split=='eval']\n",
    "\n",
    "    configs = get_configs()\n",
    "    results = []\n",
    "\n",
    "    for config in configs:\n",
    "        X_train, y_train = prepare_data(train_docs, embeddings, config)\n",
    "        cv_res, base_res, scaler = cross_validate_model(X_train, y_train)\n",
    "\n",
    "        res = {\n",
    "            'config': config['name'],\n",
    "            'cv_acc': cv_res['test_accuracy'].mean(),\n",
    "            'cv_f1': cv_res['test_f1_macro'].mean(),\n",
    "            'baseline_acc': base_res['test_accuracy'].mean(),\n",
    "            'improvement': cv_res['test_accuracy'].mean() - base_res['test_accuracy'].mean(),\n",
    "            'scaler': scaler,\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train\n",
    "        }\n",
    "        results.append(res)\n",
    "        print(f\"{config['name']}: CV Acc={res['cv_acc']:.3f}, F1={res['cv_f1']:.3f}, +{res['improvement']:.3f}\")\n",
    "\n",
    "    best = max(results, key=lambda x: x['cv_f1'])\n",
    "    print(f\"\\nBest config: {best['config']}\")\n",
    "\n",
    "    # Training finale e test\n",
    "    X_train_scaled = best['scaler'].fit_transform(best['X_train'])\n",
    "    final_model = LinearSVC(dual=False, max_iter=5000, class_weight='balanced', random_state=42)\n",
    "    final_model.fit(X_train_scaled, best['y_train'])\n",
    "\n",
    "    for dataset, docs in [('Test', test_docs), ('Eval', eval_docs)]:\n",
    "        if docs:\n",
    "            X = np.array([extract_features(d, embeddings, next(c for c in configs if c['name']==best['config'])) for d in docs])\n",
    "            y = np.array([d.author for d in docs])\n",
    "            X_scaled = best['scaler'].transform(X)\n",
    "            pred = final_model.predict(X_scaled)\n",
    "            acc = accuracy_score(y, pred)\n",
    "            f1 = f1_score(y, pred, average='macro')\n",
    "            print(f\"{dataset} - Acc={acc:.3f}, F1={f1:.3f}\")\n",
    "            print(classification_report(y, pred))\n",
    "\n",
    "    return best, final_model\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    dataset_path = \"/content/drive/MyDrive/dataset_authorship_finale\"\n",
    "    documents = load_documents(dataset_path)\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    ft_model = fasttext.load_model(\"/content/drive/MyDrive/fasttext_models/cc.en.300.bin\")\n",
    "    vocab = extract_vocab(documents)\n",
    "    embeddings = create_embeddings_dict(ft_model, vocab)\n",
    "\n",
    "    best_config, final_model = run_experiment(documents, embeddings)\n",
    "    print(f\"\\n‚úÖ Experiment finished! Best config: {best_config['config']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOvfKyDTy1TI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756740664796,
     "user_tz": -120,
     "elapsed": 194675,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "c1725f7c-feea-42e8-8390-cf1765c6d1ec"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 6492 documents\n",
      "Mean_All_Words: CV Acc=0.785, F1=0.781, +0.400\n",
      "Max_Pooling: CV Acc=0.685, F1=0.679, +0.301\n",
      "Sum_All_Words: CV Acc=0.788, F1=0.784, +0.403\n",
      "\n",
      "Best config: Sum_All_Words\n",
      "Test - Acc=0.859, F1=0.856\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  primo autore       0.92      0.84      0.88       213\n",
      "secondo autore       0.77      0.93      0.84       141\n",
      "  terzo autore       0.88      0.81      0.85       129\n",
      "\n",
      "      accuracy                           0.86       483\n",
      "     macro avg       0.86      0.86      0.86       483\n",
      "  weighted avg       0.87      0.86      0.86       483\n",
      "\n",
      "Eval - Acc=0.860, F1=0.855\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  primo autore       0.93      0.84      0.88       213\n",
      "secondo autore       0.82      0.92      0.86       142\n",
      "  terzo autore       0.81      0.83      0.82       129\n",
      "\n",
      "      accuracy                           0.86       484\n",
      "     macro avg       0.85      0.86      0.86       484\n",
      "  weighted avg       0.86      0.86      0.86       484\n",
      "\n",
      "\n",
      "‚úÖ Experiment finished! Best config: Sum_All_Words\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1DAIFuQnDKe3TgPcMzOP-ihQL8Ia7OFLQ",
     "timestamp": 1756819458507
    },
    {
     "file_id": "1FVxH5Yvu1tTatakY5FsvgXDJXvuVZKf6",
     "timestamp": 1755701406850
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
