{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Caricamento libri\n"
   ],
   "metadata": {
    "id": "cWjP7AbSguRx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx8AKCuZZWn9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757069503698,
     "user_tz": -120,
     "elapsed": 1040,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "14757afc-af8e-4a5d-d512-627bdd072a83"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ijdKI05sOxog",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1d735a69-c569-4e9c-8aba-f90200823854",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757071495228,
     "user_tz": -120,
     "elapsed": 198,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "primo autore: 5 libri, 2,274,448 caratteri\n",
      "secondo autore: 7 libri, 3,173,605 caratteri\n",
      "terzo autore: 8 libri, 2,208,855 caratteri\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def estrai_contenuto_gutenberg(testo):\n",
    "    pattern_start = r'\\*\\*\\* START OF (?:THE |THIS )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*'\n",
    "    pattern_end = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK '\n",
    "\n",
    "    match_start = re.search(pattern_start, testo, re.IGNORECASE | re.DOTALL)\n",
    "    match_end = re.search(pattern_end, testo, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if match_start and match_end:\n",
    "        return testo[match_start.end():match_end.start()].strip()\n",
    "    elif match_start:\n",
    "        return testo[match_start.end():].strip()\n",
    "    elif match_end:\n",
    "        return testo[:match_end.start()].strip()\n",
    "    else:\n",
    "        return testo\n",
    "\n",
    "def carica_tutti_i_libri(path_input, cartella_output):\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    # Crea cartella output se non esistente\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "\n",
    "    libri_raggruppati = {autore: [] for autore in autori}\n",
    "\n",
    "    for autore in autori:\n",
    "        cartella = f\"{path_input}{autore}\"\n",
    "\n",
    "        try:\n",
    "            file_txt = [f for f in os.listdir(cartella)\n",
    "                       if f.endswith('.txt')]\n",
    "\n",
    "            if not file_txt:\n",
    "                continue\n",
    "\n",
    "            file_txt.sort()\n",
    "\n",
    "            for nome_file in file_txt:\n",
    "                percorso_completo = os.path.join(cartella, nome_file)\n",
    "\n",
    "                try:\n",
    "                    with open(percorso_completo, \"r\", encoding=\"utf-8\") as f:\n",
    "                        testo_completo = f.read()\n",
    "\n",
    "                    contenuto_libro = estrai_contenuto_gutenberg(testo_completo)\n",
    "\n",
    "                    if len(contenuto_libro) > 100:\n",
    "                        libri_raggruppati[autore].append(contenuto_libro)\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        except (FileNotFoundError, Exception):\n",
    "            continue\n",
    "\n",
    "    # Salva file combinati\n",
    "    for autore, libri_autore in libri_raggruppati.items():\n",
    "        if libri_autore:\n",
    "            nome_file = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "            percorso_file = os.path.join(cartella_output, nome_file)\n",
    "\n",
    "            with open(percorso_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(libri_autore))\n",
    "\n",
    "    return libri_raggruppati\n",
    "\n",
    "# Esecuzione\n",
    "path_trainings = \"/content/drive/MyDrive/libri_training/\"\n",
    "cartella_output = \"/content/drive/MyDrive/libri_puliti_training\"\n",
    "\n",
    "libri_raggruppati = carica_tutti_i_libri(path_trainings, cartella_output)\n",
    "\n",
    "# Riepilogo essenziale\n",
    "for autore, libri in libri_raggruppati.items():\n",
    "    if libri:\n",
    "        caratteri_totali = sum(len(libro) for libro in libri)\n",
    "        print(f\"{autore}: {len(libri)} libri, {caratteri_totali:,} caratteri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funzione di estrazione paragrafi\n"
   ],
   "metadata": {
    "id": "vkpC7s62gsPU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Preparazione dati: funzioni varie"
   ],
   "metadata": {
    "id": "0YwuLqQt1dBA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def crea_corpus_word_embedding(\n",
    "    cartella_libri_puliti_training,\n",
    "    cartella_libri_puliti_test_val,\n",
    "    cartella_output_finale,\n",
    "    min_words_embedding=50,\n",
    "    max_words_embedding=100,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea corpus unificato per word embedding FastText\n",
    "    Include tutti i paragrafi di tutti gli autori (più permissivo sui limiti)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CREAZIONE CORPUS per WORD EMBEDDING\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Parametri: {min_words_embedding}-{max_words_embedding} parole per paragrafo\")\n",
    "\n",
    "    # Cartella per i file di embedding\n",
    "    cartella_embedding = os.path.join(cartella_output_finale, \"word_embedding_corpus\")\n",
    "    os.makedirs(cartella_embedding, exist_ok=True)\n",
    "\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    # Lista per raccogliere tutti i paragrafi\n",
    "    tutti_paragrafi = []\n",
    "    paragrafi_per_autore = {}\n",
    "\n",
    "    # Processa entrambe le cartelle (training + test/val)\n",
    "    cartelle_source = [\n",
    "        (\"training\", cartella_libri_puliti_training),\n",
    "        (\"test_val\", cartella_libri_puliti_test_val)\n",
    "    ]\n",
    "\n",
    "    for tipo_set, cartella_source in cartelle_source:\n",
    "        print(f\"\\n  Processando set {tipo_set}:\")\n",
    "\n",
    "        for autore in autori:\n",
    "            nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "            file_source = os.path.join(cartella_source, nome_file_autore)\n",
    "\n",
    "            if os.path.exists(file_source):\n",
    "                # Estrai paragrafi con limiti più permissivi\n",
    "                paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                    file_source,\n",
    "                    min_words=min_words_embedding,\n",
    "                    max_words=max_words_embedding,\n",
    "                    debug=debug\n",
    "                )\n",
    "\n",
    "                print(f\"    {autore} ({tipo_set}): {len(paragrafi_autore)} paragrafi\")\n",
    "\n",
    "                # Aggiungi alla collezione totale\n",
    "                tutti_paragrafi.extend(paragrafi_autore)\n",
    "\n",
    "                # Tieni traccia per autore\n",
    "                if autore not in paragrafi_per_autore:\n",
    "                    paragrafi_per_autore[autore] = []\n",
    "                paragrafi_per_autore[autore].extend(paragrafi_autore)\n",
    "            else:\n",
    "                print(f\"    {autore} ({tipo_set}): File non trovato\")\n",
    "\n",
    "    # Salva corpus unificato per FastText\n",
    "    print(f\"\\nSalvataggio corpus per word embedding:\")\n",
    "\n",
    "    # 1. Corpus completo (tutti gli autori insieme)\n",
    "    file_corpus_completo = os.path.join(cartella_embedding, \"corpus_completo.txt\")\n",
    "    with open(file_corpus_completo, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paragrafo in tutti_paragrafi:\n",
    "            # Ogni paragrafo su una riga (standard per FastText)\n",
    "            f.write(paragrafo.replace('\\n', ' ') + '\\n')\n",
    "\n",
    "    print(f\"  ✓ Corpus completo: {len(tutti_paragrafi)} paragrafi → corpus_completo.txt\")\n",
    "\n",
    "    # 2. Corpus separati per autore (opzionale, utile per analisi)\n",
    "    for autore, paragrafi in paragrafi_per_autore.items():\n",
    "        nome_file_autore_corpus = f\"corpus_{autore.replace(' ', '_')}.txt\"\n",
    "        file_corpus_autore = os.path.join(cartella_embedding, nome_file_autore_corpus)\n",
    "\n",
    "        with open(file_corpus_autore, \"w\", encoding=\"utf-8\") as f:\n",
    "            for paragrafo in paragrafi:\n",
    "                f.write(paragrafo.replace('\\n', ' ') + '\\n')\n",
    "\n",
    "        print(f\"  ✓ {autore}: {len(paragrafi)} paragrafi → {nome_file_autore_corpus}\")\n",
    "\n",
    "    # 3. Salva statistiche\n",
    "    tutte_le_parole = []\n",
    "    for paragrafo in tutti_paragrafi:\n",
    "        parole = re.findall(r'\\b\\w+\\b', paragrafo.lower())\n",
    "        tutte_le_parole.extend(parole)\n",
    "\n",
    "    vocabulary_unico = set(tutte_le_parole)\n",
    "\n",
    "    file_stats = os.path.join(cartella_embedding, \"corpus_statistics.txt\")\n",
    "    with open(file_stats, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"STATISTICHE CORPUS WORD EMBEDDING\\n\")\n",
    "        f.write(f\"=====================================\\n\\n\")\n",
    "        f.write(f\"Parametri estrazione: {min_words_embedding}-{max_words_embedding} parole/paragrafo\\n\\n\")\n",
    "        f.write(f\"TOTALI:\\n\")\n",
    "        f.write(f\"  - Paragrafi: {len(tutti_paragrafi):,}\\n\")\n",
    "        f.write(f\"  - Parole totali: {len(tutte_le_parole):,}\\n\")\n",
    "        f.write(f\"  - Vocabulary unico: {len(vocabulary_unico):,}\\n\\n\")\n",
    "        f.write(f\"PER AUTORE:\\n\")\n",
    "        for autore, paragrafi in paragrafi_per_autore.items():\n",
    "            f.write(f\"  - {autore}: {len(paragrafi):,} paragrafi\\n\")\n",
    "\n",
    "    print(f\"\\nStatistiche vocabulary:\")\n",
    "    print(f\"  - Paragrafi totali: {len(tutti_paragrafi):,}\")\n",
    "    print(f\"  - Parole totali: {len(tutte_le_parole):,}\")\n",
    "    print(f\"  - Vocabulary unico: {len(vocabulary_unico):,}\")\n",
    "\n",
    "    return cartella_embedding\n",
    "\n",
    "def conta_parole_correttamente(testo):\n",
    "    \"\"\"Conta le parole in modo accurato usando regex\"\"\"\n",
    "    if not testo or not testo.strip():\n",
    "        return 0\n",
    "\n",
    "    # Rimuovi spazi extra e normalizza\n",
    "    testo_pulito = re.sub(r'\\s+', ' ', testo.strip())\n",
    "\n",
    "    # Usa regex per trovare sequenze di caratteri alfanumerici\n",
    "    parole = re.findall(r'\\b\\w+\\b', testo_pulito)\n",
    "\n",
    "    return len(parole)\n",
    "\n",
    "def pulisci_paragrafo(paragrafo):\n",
    "    \"\"\"Pulisce un paragrafo mantenendo la leggibilità\"\"\"\n",
    "    if not paragrafo:\n",
    "        return \"\"\n",
    "\n",
    "    # Rimuovi caratteri di controllo\n",
    "    paragrafo = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', paragrafo)\n",
    "\n",
    "    # Normalizza spazi multipli\n",
    "    paragrafo = re.sub(r'\\s+', ' ', paragrafo)\n",
    "\n",
    "    # Rimuovi spazi all'inizio e fine\n",
    "    paragrafo = paragrafo.strip()\n",
    "\n",
    "    # Rimuovi linee che sono solo punteggiatura o numeri\n",
    "    if re.match(r'^[^\\w]*$', paragrafo) or re.match(r'^\\d+\\.?\\s*$', paragrafo):\n",
    "        return \"\"\n",
    "\n",
    "    return paragrafo\n",
    "\n",
    "def estrai_paragrafi_da_file_corretto(file_path, min_words=50, max_words=100, debug=False):\n",
    "    \"\"\"\n",
    "    Estrae paragrafi validi da un singolo file con conteggio parole accurato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            testo = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore leggendo {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if debug:\n",
    "        print(f\"File: {os.path.basename(file_path)}\")\n",
    "        print(f\"Dimensione originale: {len(testo)} caratteri\")\n",
    "\n",
    "    # Gestisci diversi tipi di separatori di paragrafo\n",
    "    # Prima normalizza i line break\n",
    "    testo = testo.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Identifica paragrafi (linee vuote multiple = separatore paragrafo)\n",
    "    paragrafi_raw = re.split(r'\\n\\s*\\n', testo)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Paragrafi raw trovati: {len(paragrafi_raw)}\")\n",
    "\n",
    "    paragrafi_validi = []\n",
    "    stats = {'troppo_corti': 0, 'troppo_lunghi': 0, 'validi': 0, 'vuoti': 0}\n",
    "\n",
    "    for i, paragrafo_raw in enumerate(paragrafi_raw):\n",
    "        # Pulisci il paragrafo\n",
    "        paragrafo = pulisci_paragrafo(paragrafo_raw)\n",
    "\n",
    "        if not paragrafo:\n",
    "            stats['vuoti'] += 1\n",
    "            continue\n",
    "\n",
    "        # Conta parole accuratamente\n",
    "        num_parole = conta_parole_correttamente(paragrafo)\n",
    "\n",
    "        if debug and i < 5:  # Mostra primi 5 per debug\n",
    "            print(f\"Paragrafo {i+1}: {num_parole} parole - '{paragrafo[:100]}...'\")\n",
    "\n",
    "        # Filtra per lunghezza\n",
    "        if num_parole < min_words:\n",
    "            stats['troppo_corti'] += 1\n",
    "        elif num_parole > max_words:\n",
    "            stats['troppo_lunghi'] += 1\n",
    "        else:\n",
    "            paragrafi_validi.append(paragrafo)\n",
    "            stats['validi'] += 1\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Statistiche estrazione:\")\n",
    "        print(f\"  - Paragrafi validi: {stats['validi']}\")\n",
    "        print(f\"  - Troppo corti (<{min_words}): {stats['troppo_corti']}\")\n",
    "        print(f\"  - Troppo lunghi (>{max_words}): {stats['troppo_lunghi']}\")\n",
    "        print(f\"  - Vuoti/invalidi: {stats['vuoti']}\")\n",
    "\n",
    "    return paragrafi_validi\n",
    "\n",
    "def verifica_paragrafi_estratti(paragrafi, min_words=50, max_words=100):\n",
    "    \"\"\"Verifica che i paragrafi estratti rispettino i limiti\"\"\"\n",
    "    print(f\"\\nVerifica {len(paragrafi)} paragrafi:\")\n",
    "\n",
    "    problemi = []\n",
    "    for i, paragrafo in enumerate(paragrafi):\n",
    "        num_parole = conta_parole_correttamente(paragrafo)\n",
    "\n",
    "        if num_parole < min_words:\n",
    "            problemi.append(f\"Paragrafo {i+1}: {num_parole} parole (< {min_words})\")\n",
    "        elif num_parole > max_words:\n",
    "            problemi.append(f\"Paragrafo {i+1}: {num_parole} parole (> {max_words})\")\n",
    "\n",
    "    if problemi:\n",
    "        print(f\"ATTENZIONE: {len(problemi)} paragrafi fuori range:\")\n",
    "        for problema in problemi[:10]:  # Mostra max 10\n",
    "            print(f\"  - {problema}\")\n",
    "        if len(problemi) > 10:\n",
    "            print(f\"  ... e altri {len(problemi) - 10}\")\n",
    "    else:\n",
    "        print(\"Tutti i paragrafi rispettano i limiti di parole\")\n",
    "\n",
    "    # Statistiche\n",
    "    parole_per_paragrafo = [conta_parole_correttamente(p) for p in paragrafi]\n",
    "    if parole_per_paragrafo:\n",
    "        print(f\"Parole per paragrafo - Min: {min(parole_per_paragrafo)}, \"\n",
    "              f\"Max: {max(parole_per_paragrafo)}, \"\n",
    "              f\"Media: {sum(parole_per_paragrafo)/len(parole_per_paragrafo):.1f}\")\n",
    "\n",
    "def crea_struttura_dataset_finale_corretta(\n",
    "    cartella_libri_puliti_training,\n",
    "    cartella_libri_puliti_test_val,\n",
    "    cartella_output_finale,\n",
    "    min_words=50,\n",
    "    max_words=100,\n",
    "    test_size=0.5,\n",
    "    debug=False,\n",
    "    verifica_output=True,\n",
    "    crea_embedding_corpus=False  # <-- AGGIUNGI QUESTO PARAMETRO\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Versione corretta della creazione dataset con conteggio parole accurato\n",
    "    \"\"\"\n",
    "\n",
    "    # Pulisci cartella output\n",
    "    if os.path.exists(cartella_output_finale):\n",
    "        print(f\"Pulizia cartella output: {cartella_output_finale}\")\n",
    "        import shutil\n",
    "        shutil.rmtree(cartella_output_finale)\n",
    "\n",
    "   # AGGIUNGI QUESTO BLOCCO all'inizio della funzione (dopo la pulizia cartella)\n",
    "    if crea_embedding_corpus:\n",
    "        crea_corpus_word_embedding(\n",
    "            cartella_libri_puliti_training,\n",
    "            cartella_libri_puliti_test_val,\n",
    "            cartella_output_finale,\n",
    "            debug=debug\n",
    "        )\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CREAZIONE DATASET per CLASSIFICATION\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    # Crea struttura\n",
    "    sets = [\"training_set\", \"test_set\", \"eval_set\"]\n",
    "    autori = [\"primo autore\", \"secondo autore\", \"terzo autore\"]\n",
    "\n",
    "    for set_name in sets:\n",
    "        for autore in autori:\n",
    "            cartella_set_autore = os.path.join(cartella_output_finale, set_name, autore)\n",
    "            os.makedirs(cartella_set_autore, exist_ok=True)\n",
    "\n",
    "    print(f\"Struttura cartelle creata in: {cartella_output_finale}\")\n",
    "\n",
    "    # TRAINING SET\n",
    "    print(f\"\\nProcessing TRAINING SET (parole: {min_words}-{max_words}):\")\n",
    "    training_stats = {}\n",
    "\n",
    "    for autore in autori:\n",
    "        nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "        file_source = os.path.join(cartella_libri_puliti_training, nome_file_autore)\n",
    "\n",
    "        print(f\"\\n  {autore}:\")\n",
    "\n",
    "        if os.path.exists(file_source):\n",
    "            # Estrai paragrafi con nuovo metodo\n",
    "            paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                file_source,\n",
    "                min_words=min_words,\n",
    "                max_words=max_words,\n",
    "                debug=debug\n",
    "            )\n",
    "\n",
    "            if paragrafi_autore:\n",
    "                # Verifica se richiesto\n",
    "                if verifica_output:\n",
    "                    verifica_paragrafi_estratti(paragrafi_autore, min_words, max_words)\n",
    "\n",
    "                # Salva paragrafi\n",
    "                cartella_training_autore = os.path.join(cartella_output_finale, \"training_set\", autore)\n",
    "                salva_paragrafi_separati(paragrafi_autore, cartella_training_autore, \"par\")\n",
    "\n",
    "                training_stats[autore] = len(paragrafi_autore)\n",
    "                print(f\"    Training: {len(paragrafi_autore)} paragrafi salvati\")\n",
    "            else:\n",
    "                print(f\"    Nessun paragrafo valido estratto\")\n",
    "                training_stats[autore] = 0\n",
    "        else:\n",
    "            print(f\"    File non trovato: {nome_file_autore}\")\n",
    "            training_stats[autore] = 0\n",
    "\n",
    "    # TEST & EVAL SET\n",
    "    print(f\"\\nProcessing TEST & EVAL SET:\")\n",
    "    test_eval_stats = {}\n",
    "\n",
    "    for autore in autori:\n",
    "        nome_file_autore = f\"{autore.replace(' ', '_')}_tutti_i_libri.txt\"\n",
    "        file_source = os.path.join(cartella_libri_puliti_test_val, nome_file_autore)\n",
    "\n",
    "        print(f\"\\n  {autore}:\")\n",
    "\n",
    "        if os.path.exists(file_source):\n",
    "            # Estrai paragrafi\n",
    "            paragrafi_autore = estrai_paragrafi_da_file_corretto(\n",
    "                file_source,\n",
    "                min_words=min_words,\n",
    "                max_words=max_words,\n",
    "                debug=debug\n",
    "            )\n",
    "\n",
    "            if paragrafi_autore:\n",
    "                # Verifica\n",
    "                if verifica_output:\n",
    "                    verifica_paragrafi_estratti(paragrafi_autore, min_words, max_words)\n",
    "\n",
    "                # Dividi in test e eval\n",
    "                if len(paragrafi_autore) >= 2:\n",
    "                    paragrafi_test, paragrafi_eval = train_test_split(\n",
    "                        paragrafi_autore,\n",
    "                        test_size=test_size,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                else:\n",
    "                    # Se c'è solo 1 paragrafo, mettilo nel test\n",
    "                    paragrafi_test = paragrafi_autore\n",
    "                    paragrafi_eval = []\n",
    "\n",
    "                # Salva\n",
    "                cartella_test_autore = os.path.join(cartella_output_finale, \"test_set\", autore)\n",
    "                cartella_eval_autore = os.path.join(cartella_output_finale, \"eval_set\", autore)\n",
    "\n",
    "                salva_paragrafi_separati(paragrafi_test, cartella_test_autore, \"par\")\n",
    "                salva_paragrafi_separati(paragrafi_eval, cartella_eval_autore, \"par\")\n",
    "\n",
    "                test_eval_stats[autore] = {\n",
    "                    'totale': len(paragrafi_autore),\n",
    "                    'test': len(paragrafi_test),\n",
    "                    'eval': len(paragrafi_eval)\n",
    "                }\n",
    "\n",
    "                print(f\"    Totale: {len(paragrafi_autore)} → Test: {len(paragrafi_test)}, Eval: {len(paragrafi_eval)}\")\n",
    "            else:\n",
    "                print(f\"    Nessun paragrafo valido estratto\")\n",
    "                test_eval_stats[autore] = {'totale': 0, 'test': 0, 'eval': 0}\n",
    "        else:\n",
    "            print(f\"    File non trovato: {nome_file_autore}\")\n",
    "            test_eval_stats[autore] = {'totale': 0, 'test': 0, 'eval': 0}\n",
    "\n",
    "    # RIEPILOGO FINALE\n",
    "    print(f\"\\nRIEPILOGO DATASET FINALE:\")\n",
    "    print(f\"Parametri: parole per paragrafo {min_words}-{max_words}\")\n",
    "\n",
    "    # AGGIUNGI QUESTO messaggio se è stato creato il corpus\n",
    "    if crea_embedding_corpus:\n",
    "        print(f\"\\n✓ CORPUS WORD EMBEDDING creato in: word_embedding_corpus/\")\n",
    "        print(f\"  → Usa 'corpus_completo.txt' per addestrare FastText\")\n",
    "\n",
    "    print(f\"\\nTRAINING SET:\")\n",
    "    for autore, count in training_stats.items():\n",
    "        print(f\"  {autore}: {count} paragrafi\")\n",
    "\n",
    "    print(f\"\\nTEST SET:\")\n",
    "    for autore, stats in test_eval_stats.items():\n",
    "        print(f\"  {autore}: {stats['test']} paragrafi\")\n",
    "\n",
    "    print(f\"\\nEVAL SET:\")\n",
    "    for autore, stats in test_eval_stats.items():\n",
    "        print(f\"  {autore}: {stats['eval']} paragrafi\")\n",
    "\n",
    "    print(f\"\\nTOTALE PER AUTORE:\")\n",
    "    for autore in autori:\n",
    "        training_count = training_stats.get(autore, 0)\n",
    "        test_count = test_eval_stats.get(autore, {}).get('test', 0)\n",
    "        eval_count = test_eval_stats.get(autore, {}).get('eval', 0)\n",
    "        totale = training_count + test_count + eval_count\n",
    "        print(f\"  {autore}: {totale} paragrafi totali\")\n",
    "\n",
    "    return cartella_output_finale\n",
    "\n",
    "def salva_paragrafi_separati(paragrafi, cartella_output, prefisso=\"par\"):\n",
    "    \"\"\"Salva ogni paragrafo come file separato\"\"\"\n",
    "    os.makedirs(cartella_output, exist_ok=True)\n",
    "\n",
    "    for i, paragrafo in enumerate(paragrafi, 1):\n",
    "        nome_file = f\"{prefisso}-{i:03d}.txt\"  # Numerazione con zeri iniziali\n",
    "        percorso_file = os.path.join(cartella_output, nome_file)\n",
    "\n",
    "        with open(percorso_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(paragrafo)\n",
    "\n",
    "# TEST DELLA FUNZIONE\n",
    "if __name__ == \"__main__\":\n",
    "    # Esempio di uso\n",
    "    print(\"Test conteggio parole:\")\n",
    "\n",
    "    testi_test = [\n",
    "        \"Questa è una frase con esattamente otto parole totali.\",\n",
    "        \"Una   frase    con     spazi      multipli.\",\n",
    "        \"Frase, con! punteggiatura? attaccata: alle; parole.\",\n",
    "        \"   Spazi iniziali e finali   \",\n",
    "        \"\",\n",
    "        \"Una sola parola\"\n",
    "    ]\n",
    "\n",
    "    for testo in testi_test:\n",
    "        parole = conta_parole_correttamente(testo)\n",
    "        print(f\"'{testo}' → {parole} parole\")"
   ],
   "metadata": {
    "id": "Af-3qHAi5KQk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757076574352,
     "user_tz": -120,
     "elapsed": 48,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_path = crea_struttura_dataset_finale_corretta(\n",
    "    cartella_libri_puliti_training=\"/content/drive/MyDrive/libri_puliti_training\",\n",
    "    cartella_libri_puliti_test_val=\"/content/drive/MyDrive/libri_test_val_puliti\",\n",
    "    cartella_output_finale=\"/content/drive/MyDrive/dataset_authorship_finale\",\n",
    "    min_words=50,\n",
    "    max_words=100,\n",
    "    debug=True,\n",
    "    verifica_output=True,\n",
    "    crea_embedding_corpus=True\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx4YmHG05NVL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1757076645163,
     "user_tz": -120,
     "elapsed": 68127,
     "user": {
      "displayName": "pietro",
      "userId": "16162074620451566409"
     }
    },
    "outputId": "bbd3bf57-804e-4c00-fbd2-f6279b9bb183"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "CREAZIONE CORPUS WORD EMBEDDING (50-100 parole)\n",
      "==================================================\n",
      "\n",
      "  Processando training:\n",
      "    Paragrafo 2: 5 parole - '==================================================...'\n",
      "    Paragrafo 3: 17 parole - 'BY R. AUSTIN FREEMAN Author of “The Singing Bone,”...'\n",
      "    Stats - Validi: 1827, Corti: 5976, Lunghi: 1037, Vuoti: 9\n",
      "    primo autore: 1827 paragrafi\n",
      "    Paragrafo 2: 2 parole - '==================================================...'\n",
      "    Paragrafo 3: 2 parole - 'BIG LAUREL...'\n",
      "    Stats - Validi: 2125, Corti: 14505, Lunghi: 1436, Vuoti: 19\n",
      "    secondo autore: 2125 paragrafi\n",
      "    Paragrafo 2: 3 parole - '==================================================...'\n",
      "    Paragrafo 3: 6 parole - 'INDIAN AND HUNTING STORIES FOR BOYS...'\n",
      "    Stats - Validi: 1559, Corti: 6914, Lunghi: 1057, Vuoti: 21\n",
      "    terzo autore: 1559 paragrafi\n",
      "\n",
      "  Processando test_val:\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 4 parole - 'The Eye of Osiris...'\n",
      "    Paragrafo 3: 4 parole - 'A Detective Story by...'\n",
      "    Stats - Validi: 426, Corti: 1632, Lunghi: 223, Vuoti: 0\n",
      "    primo autore: 426 paragrafi\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 7 parole - '[Frontispiece: A new tenderness swept over her]...'\n",
      "    Paragrafo 3: 2 parole - 'THE TRIFLERS...'\n",
      "    Stats - Validi: 283, Corti: 2349, Lunghi: 170, Vuoti: 1\n",
      "    secondo autore: 283 paragrafi\n",
      "    Paragrafo 1: 3 parole - 'Transcriber's Note:...'\n",
      "    Paragrafo 2: 38 parole - 'Every effort has been made to replicate this text ...'\n",
      "    Paragrafo 3: 20 parole - 'Italic text has been marked with _underscores_. Bo...'\n",
      "    Stats - Validi: 258, Corti: 1013, Lunghi: 170, Vuoti: 2\n",
      "    terzo autore: 258 paragrafi\n",
      "\n",
      "✓ Corpus salvato: 6478 paragrafi, vocabulary: 20,323\n",
      "  → Usa 'corpus_completo.txt' per addestrare FastText\n",
      "\n",
      "==================================================\n",
      "CREAZIONE DATASET CLASSIFICATION (50-100 parole)\n",
      "==================================================\n",
      "\n",
      "Processing TRAINING SET:\n",
      "    Paragrafo 2: 5 parole - '==================================================...'\n",
      "    Paragrafo 3: 17 parole - 'BY R. AUSTIN FREEMAN Author of “The Singing Bone,”...'\n",
      "    Stats - Validi: 1827, Corti: 5976, Lunghi: 1037, Vuoti: 9\n",
      "    primo autore: 1827 paragrafi\n",
      "    Paragrafo 2: 2 parole - '==================================================...'\n",
      "    Paragrafo 3: 2 parole - 'BIG LAUREL...'\n",
      "    Stats - Validi: 2125, Corti: 14505, Lunghi: 1436, Vuoti: 19\n",
      "    secondo autore: 2125 paragrafi\n",
      "    Paragrafo 2: 3 parole - '==================================================...'\n",
      "    Paragrafo 3: 6 parole - 'INDIAN AND HUNTING STORIES FOR BOYS...'\n",
      "    Stats - Validi: 1559, Corti: 6914, Lunghi: 1057, Vuoti: 21\n",
      "    terzo autore: 1559 paragrafi\n",
      "\n",
      "Processing TEST & EVAL SET:\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 4 parole - 'The Eye of Osiris...'\n",
      "    Paragrafo 3: 4 parole - 'A Detective Story by...'\n",
      "    Stats - Validi: 426, Corti: 1632, Lunghi: 223, Vuoti: 0\n",
      "    primo autore: 426 paragrafi\n",
      "    Paragrafo 1: 4 parole - 'Produced by Al Haines...'\n",
      "    Paragrafo 2: 7 parole - '[Frontispiece: A new tenderness swept over her]...'\n",
      "    Paragrafo 3: 2 parole - 'THE TRIFLERS...'\n",
      "    Stats - Validi: 283, Corti: 2349, Lunghi: 170, Vuoti: 1\n",
      "    secondo autore: 283 paragrafi\n",
      "    Paragrafo 1: 3 parole - 'Transcriber's Note:...'\n",
      "    Paragrafo 2: 38 parole - 'Every effort has been made to replicate this text ...'\n",
      "    Paragrafo 3: 20 parole - 'Italic text has been marked with _underscores_. Bo...'\n",
      "    Stats - Validi: 258, Corti: 1013, Lunghi: 170, Vuoti: 2\n",
      "    terzo autore: 258 paragrafi\n",
      "    primo autore: 426 totali → Test: 213, Eval: 213\n",
      "    secondo autore: 283 totali → Test: 141, Eval: 142\n",
      "    terzo autore: 258 totali → Test: 129, Eval: 129\n",
      "\n",
      "RIEPILOGO FINALE:\n",
      "✓ CORPUS EMBEDDING: word_embedding_corpus/corpus_completo.txt\n",
      "\n",
      "TRAINING SET:\n",
      "  primo autore: 1827 paragrafi\n",
      "  secondo autore: 2125 paragrafi\n",
      "  terzo autore: 1559 paragrafi\n",
      "\n",
      "TEST SET:\n",
      "  primo autore: 213 paragrafi\n",
      "  secondo autore: 141 paragrafi\n",
      "  terzo autore: 129 paragrafi\n",
      "\n",
      "EVAL SET:\n",
      "  primo autore: 213 paragrafi\n",
      "  secondo autore: 142 paragrafi\n",
      "  terzo autore: 129 paragrafi\n",
      "\n",
      "TOTALE PER AUTORE:\n",
      "  primo autore: 2253 paragrafi totali\n",
      "  secondo autore: 2408 paragrafi totali\n",
      "  terzo autore: 1817 paragrafi totali\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1DAIFuQnDKe3TgPcMzOP-ihQL8Ia7OFLQ",
     "timestamp": 1756819458507
    },
    {
     "file_id": "1FVxH5Yvu1tTatakY5FsvgXDJXvuVZKf6",
     "timestamp": 1755701406850
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}